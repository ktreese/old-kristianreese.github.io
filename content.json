{"meta":{"title":"kristianreese.com","subtitle":"new home of kb.kristianreese.com","description":"My personal, yet public knowledgebase","author":"Kris Reese","url":"http://kristianreese.com","root":"/"},"pages":[],"posts":[{"title":"Monitor Log for pattern match and trigger an action","slug":"Monitor-Log-for-pattern-match-and-trigger-an-action","date":"2019-07-11T08:11:00.000Z","updated":"2019-07-10T22:15:14.000Z","comments":true,"path":"2019/07/11/Monitor-Log-for-pattern-match-and-trigger-an-action/","link":"","permalink":"http://kristianreese.com/2019/07/11/Monitor-Log-for-pattern-match-and-trigger-an-action/","excerpt":"","text":"For real time log monitoring in which an action should be triggered, “awk program execution” can be used. An AWK program consists of a sequence of pattern-action statements and optional function definitions. pattern { action statements } From man page: For each record in the input, gawk tests to see if it matches any pattern in the AWK program. For each pattern that the record matches, the associated action is executed. The patterns are tested in the order they occur in the program. Examples to utilize this on a command line would look like this: ~$ tail -fn0 logfile | awk &#39;/pattern/ { print }&#39; where print will ouput the pattern the console. print can be replaced with any command, even to call another shell script. If the command contains spaces, use double quotes, for example: ~$ tail -fn0 logfile | awk &#39;/pattern/ { &quot;echo ${VARIABLE}&quot; }&#39; This method works great if there is a need for continuous log monitoring and action triggering any time pattern is found. But what to do if there is a need to run this inside a shell script? I attempted to do varous things from using the action statement “exit”, to command concatenation like “print &amp;&amp; exit“ or “print; exit“, but every time tail would not recognize the pipe had been broken and therefore would not exit. This precluded the bash script from moving onto the next block of code to execute once pattern was found. So how do we do this? How to terminate tail -f inside a shell scriptTo reach a solution, I began by observing the differences in running the following tail commands with the action statement set to “exit” via command line, and from a shell script: tail -fn0 server.log | awk &#39;/\\(JBoss Shutdown Hook\\) Shutdown complete/ { exit }&#39; ps -ef output when running from command line:reese 5922 4181 0 13:04 pts/1 00:00:00 tail -fn0 server.log ps -ef output when running from shell script:reese 5591 4100 0 12:59 pts/0 00:00:00 /bin/bash ./deployEar.sh reese 5641 5591 0 12:59 pts/0 00:00:00 tail -fn0 /opt/Jboss_5.1.2/server/JbossServer1/log/server.log reese 5642 5591 0 12:59 pts/0 00:00:00 awk /\\(JBoss Shutdown Hook\\) Shutdown complete/ &#123; exit &#125; Interesting that awk doesn’t show up as a separate process when running from command line. This must be why tail exits once the pattern is logged. However, from the shell script, only the awk process exits when “exit” is executed, leaving tail to run and therefore hanging up the shell script from further code execution. To solve this issue, the tail man page has the following option:--pid=PID with -f, terminate after process ID, PID dies Awesome! So let’s use this to tell it to terminate tail -f after awk dies, like this! tail -fn0 --pid=$(pidof awk) server.log | awk &#39;/\\(JBoss Shutdown Hook\\) Shutdown complete/ { exit }&#39; Works like a charm. I hope this helps. An example of what I used this for was to monitor an application log for a shutdown trigger. If stopped, sleep 2 seconds then start the application. $&#123;BINPATH&#125;/application.sh stop &gt;/dev/null 2&gt;&amp;1 tail -fn0 --pid=$(pidof awk) $&#123;APPLOG&#125; | awk &apos;/\\(JBoss Shutdown Hook\\) Shutdown complete/ &#123; exit &#125;&apos; tac $&#123;APPLOG&#125; | grep -m1 &quot;Shutdown complete&quot; | awk &apos;&#123; print $1&quot; &quot;$2&quot; &quot;$5&quot; &quot;$6&quot; &quot;$7&quot; &quot;$8&quot; &quot;$9 &#125;&apos; sleep 2 echo -e &quot;\\nStarting $&#123;Target&#125; on $&#123;HOSTNAME&#125;. This will take a few minutes...&quot; $&#123;BINPATH&#125;/application.sh start &gt;/dev/null 2&gt;&amp;1 tail -fn0 --pid=$(pidof awk) $&#123;APPLOG&#125; | awk &apos;/Started in/ &#123; exit &#125;&apos; tac $&#123;APPLOG&#125; | grep -m1 &quot;Started in&quot; | awk &apos;&#123; print $1&quot; &quot;$2&quot; &quot;$12&quot; &quot;$13&quot; &quot;$14 &#125;&apos; If using as an alias within .bash_profile, escape $ where appropriate. For example: alias applog=&quot;tail -fn0 --pid=\\$(pidof awk) ${APPLOG} | awk &#39;/Started in/ { exit }&#39;; tac ${APPLOG} | grep -m1 &#39;Started in&#39; | awk &#39;{ print \\$1\\&quot; \\&quot;\\$2\\&quot; \\&quot;\\$12\\&quot; \\&quot;\\$13\\&quot; \\&quot;\\$14 }&#39;&quot; or wrap it into a function within .bash_profile: function app()&#123; ENV=$(hostname | awk &apos;BEGIN &#123;FS=&quot;scm&quot;&#125;&#123;print $1&#125;&apos;) ENV=$(echo $ENV | awk &apos;&#123; print toupper($0)&#125;&apos;) if [ &quot;$1&quot; = &quot;sct&quot; ]; then #LOG=&quot;/scmlogs/$&#123;ENV&#125;/$&#123;HOSTNAME&#125;/SCTServer1/log/server.log&quot; LOG=&quot;/opt/Jboss_5.1.2/server/SCTServer1/log/server.log&quot; CMD=&quot;/opt/Jboss_5.1.2/bin/sct.sh&quot; elif [ &quot;$1&quot; = &quot;clm&quot; ]; then #LOG=&quot;/scmlogs/$&#123;ENV&#125;/$&#123;HOSTNAME&#125;/CLMServer1/log/server.log&quot; LOG=&quot;/opt/Jboss_5.1.2/server/CLMServer1/log/server.log&quot; CMD=&quot;/opt/Jboss_5.1.2/bin/clm.sh&quot; elif [ &quot;$1&quot; = &quot;sctbatch&quot; ]; then #LOG=&quot;/scmlogs/$&#123;ENV&#125;/$&#123;HOSTNAME&#125;/CLMServer1/log/server.log&quot; LOG=&quot;/opt/Jboss_5.1.2/server/SCTBatchServer/log/server.log&quot; CMD=&quot;/opt/Jboss_5.1.2/bin/sctbatch.sh&quot; else echo &quot;Usage: app [sct|clm|sctbatch] [start|stop|status] [tail]&quot; return 1; fi case $2 in start) eval $&#123;CMD&#125; start if [ ! -z $3 ]; then tail -f $&#123;LOG&#125; else tail -fn0 --pid=$(pidof awk) $&#123;LOG&#125; | awk &apos;/Started in/ &#123; exit &#125;&apos; fi tac $&#123;LOG&#125; | grep -m1 &apos;Started in&apos; | awk &apos;&#123; print $1&quot; &quot;$2&quot; &quot;$12&quot; &quot;$13&quot; &quot;$14 &#125;&apos; ;; stop) eval $&#123;CMD&#125; stop if [ ! -z $3 ]; then tail -f $&#123;LOG&#125; else tail -fn0 --pid=$(pidof awk) $&#123;LOG&#125; | awk &apos;/\\(JBoss Shutdown Hook\\) Shutdown complete/ &#123; exit &#125;&apos; fi echo &quot;Shutdown Complete&quot; ;; status) eval $&#123;CMD&#125; status ;; *) echo &quot;Usage: app [sct|clm|sctbatch] [start|stop|status] [tail]&quot; ;; esac&#125; END","categories":[{"name":"Scripting","slug":"Scripting","permalink":"http://kristianreese.com/categories/Scripting/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Run RabbitMQ as a non-root user","slug":"Run-RabbitMQ-as-a-non-root-user","date":"2019-07-11T08:04:00.000Z","updated":"2019-07-10T22:08:22.000Z","comments":true,"path":"2019/07/11/Run-RabbitMQ-as-a-non-root-user/","link":"","permalink":"http://kristianreese.com/2019/07/11/Run-RabbitMQ-as-a-non-root-user/","excerpt":"","text":"By default, the provided Fedora / RHEL RabbitMQ RPM creates a local user account called ‘rabbitmq’. However, the app needs to be started by the root user, which starts the service as the rabbitmq user. As far as I can tell, this is required because lock and pid files are created in root owned directories. However, since RabbitMQ, by default, does not run on a privledged port, we can customize the installation process of RabbitMQ and set it up to start, stop, and run as any user we wish. My approach was to create my own RPM package from RabbitMQ binary tar.gz. Create custom RabbitMQ RPM Package In order to create our own RabbitMQ RPM package, the following packages need to exist on the system: rpm-build redhat-rpm-config Typically, in an rpmbuild scenario, we would also require a complier like gcc and make, but in this example, we will not be compling code since we’re going to make an RPM out of a binary distribution of files provided to us in a tar.gz. Prepare rpmbuild environment The first order of business is to prepare the rpmbuild directory structure. There are two ways to do this: FirstThis is as easy as creating a few subdirectories along with a .rpmmacros file to contain directives not typically found in a .spec file. DEV mquser@rabbitmq01 ~ $ echo &apos;%_topdir %(echo $HOME)/rpmbuild&apos; &gt; ~/.rpmmacros DEV mquser@rabbitmq01 ~ $ mkdir -p ~/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125; Alternatively, install the rpmdevtools package and as a non-root user (in this example the mquser), run the command rpmdev-setuptree. SecondAlternatively, one may wish to implement customizations from the src RPM, whereby upon installation of the src RPM, the rpmbuild directory structure is created for you along with the source files and spec file used to create the RPM. In order to do this, first create the .rpmmacros file as in the First option above, then download the src RabbitMQ RPM and install it from the home dir of the non-root user being used to build the custom RPM. DEV mquser@rabbitmq01 ~ $ cd $HOME DEV mquser@rabbitmq01 ~ $ wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.3.3/rabbitmq-server-3.3.3-1.src.rpm DEV mquser@rabbitmq01 ~ $ DEV mquser@rabbitmq01 ~ $ rpm -ivh rabbitmq-server-3.3.3-1.src.rpm&lt;/div&gt; If the second option is exercised, the rpmbuild and subsequent directories are put into place. Navigate around and check them out. For our purposes, we do not require everything the src RPM provides us, therefore, delete everything from the SOURCES directory except for rabbitmq-server.init Download RabbitMQ generic unix tar ball Now that the rpmbuild directory structure is in place, it’s time to download the binary. DEV mquser@rabbitmq01 ~ $ cd /export/home/mquser/rpmbuild/SOURCES DEV mquser@rabbitmq01 ~ $ wget https://www.rabbitmq.com/releases/rabbitmq-server/v3.3.3/rabbitmq-server-generic-unix-3.3.3.tar.gz Extract the contents, rename the extracted directory, and re-tar. This is necessary in order to keep the name schema consistent with how we will define our directives within the spec file. DEV mquser@rabbitmq01 ~ $ tar zxvf rabbitmq-server-generic-unix-3.3.3.tar.gz DEV mquser@rabbitmq01 ~ $ mv rabbitmq_server-3.3.3 rabbitmq-3.3.3 DEV mquser@rabbitmq01 ~ $ tar cvf - ./rabbitmq-3.3.3 | gzip &gt; rabbitmq-3.3.3.tar.gz DEV mquser@rabbitmq01 ~ $ rm –rf ./rabbitmq-3.3.3 Create / modify the spec fileThe spec file is kept in the ~/rpmbuild/SPECS directory and is what allows the custom definitions we’re after in allowing the start / stop of the RabbitMQ application as a non-root user. It is usually a good idea to model the spec file from the src RPM spec file. This can be obtained by installing the src RPM as a non-root user as described in the Second option above, and modifying the resulting spec file from the ~/rpmbuild/SPECS directory. Otherwise, a spec file can be created manually as shown here. Name: rabbitmqVersion: 3.3.3Release: 1%&#123;?dist&#125;Summary: RabbitMQ custom RPM build for mquserGroup: Development/LibrariesLicense: MPLv1.1 and MIT and ASL 2.0 and BSDURL: http://www.rabbitmq.com/Source0: %&#123;name&#125;-%&#123;version&#125;.tar.gzSource1: rabbitmq-server.initBuildArch: noarchBuildRoot: %(mktemp -ud %&#123;_tmppath&#125;/%&#123;name&#125;-%&#123;version&#125;-%&#123;release&#125;-XXXXXX)BuildRequires: erlang &gt;= R13B-03Requires: erlang &gt;= R13B-03, logrotateRequires(post): chkconfig initscriptsRequires(pre): chkconfig initscripts%descriptionRabbitMQ is an implementation of AMQP, the emerging standard for highperformance enterprise messaging. The RabbitMQ server is a robust andscalable implementation of an AMQP broker.# Install RabbitMQ to /opt%define _prefix /opt/%prep%setup -q%build%installrm -rf %&#123;buildroot&#125;mkdir -m 0755 -p %&#123;buildroot&#125;%&#123;_prefix&#125;%&#123;name&#125;mkdir -m 0755 -p %&#123;buildroot&#125;%&#123;_prefix&#125;%&#123;name&#125;%&#123;_localstatedir&#125;mkdir -m 0755 -p %&#123;buildroot&#125;%&#123;_prefix&#125;%&#123;name&#125;%&#123;_localstatedir&#125;/lock/subsyscp -R * %&#123;buildroot&#125;%&#123;_prefix&#125;%&#123;name&#125;#Copy all necessary start up, logrotate config files etc.install -p -D -m 0755 %&#123;S:1&#125; %&#123;buildroot&#125;%&#123;_initrddir&#125;/rabbitmq-server%cleanrm -rf %&#123;buildroot&#125;%postif [ $1 = 1 ]; then #rpm -ivh issued for initial installation echo &quot;Enabling rabbitmq-management plugin&quot; su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmq-plugins enable rabbitmq_management &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmq-server -detached &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser sleep 2 echo &quot;Creating user accounts...&quot; su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl add_user mqadmin password &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl set_user_tags mqadmin administrator &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl set_permissions -p / mqadmin \\&quot;.*\\&quot; \\&quot;.*\\&quot; \\&quot;.*\\&quot; &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl add_user mqreadonly password &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl set_user_tags mqreadonly management &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl set_permissions -p / mqreadonly \\&quot;\\&quot; \\&quot;\\&quot; \\&quot;.*\\&quot; &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser echo &quot;Deleting guest user account&quot; su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl delete_user guest &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser echo &quot;Shutting down server&quot; su -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl stop &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquserfichown -R mquser:root %&#123;_prefix&#125;%&#123;name&#125;/%&#123;_localstatedir&#125;*echo &quot;Please start RabbitMQ as mquser via &apos;/etc/init.d/rabbitmq-server start&apos;&quot;%preunsu -c &quot;%&#123;_prefix&#125;%&#123;name&#125;/sbin/rabbitmqctl stop &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser%postunif [ $1 = 0 ]; then #service is stopped in the preun section #Complete uninstall su -c &quot;kill -9 `pidof epmd` &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquser su -c &quot;rm -rf %&#123;_prefix&#125;%&#123;name&#125; &gt; /dev/null 2&gt;&amp;1 ||:&quot; mquserfi%files%defattr(0755,mquser,root,-)%&#123;_prefix&#125;%&#123;name&#125;/*%&#123;_initrddir&#125;/rabbitmq-server%dir %&#123;_prefix&#125;%&#123;name&#125;%dir %&#123;_prefix&#125;%&#123;name&#125;/%&#123;_localstatedir&#125;%doc%changelog* Mon Jun 18 2014 Kris Reese- Initial creation Description of Changes / Items of note Line Item 1: Change Name to ‘rabbitmq’ Line Item 9: Change Source0 to what’s shown Line Item 10: Change rabbitmq-server.init Source definition as Source1 get rid of all other Source definitions Line Item 14: modify as shown Litem Item 15: modify as shown Line Item 25: Define prefix as /opt as this is where rabbitmq will be installed, and things like the lock and pid files will be kept. Litem Items 34 thru 37: Define as shown. Some directories have to be made as part of the build as they’re not part of the binary tar. %post section Check if this is an initial installation – if [ $1 = 1 ] as we don’t want to run %post for an upgrade event If initial, enable the rabbitmq-managment plugin as part of the installation process, create user accounts, and delete guest user set permissions to the mquser %preun and %postun set steps for uninstall Modify rabbitmq-server.init scriptThere are only a few lines that require modification from the rabbitmq-server.init script provided via the src RPM, and that is to add “/opt/rabbitmq” to the defined PATH, DAEMON, CONTROL, INIT_LOG_DIR, and PID_FILE variables. Also, add GROUP=users and change USER=mquser. Build the RPMDEV mquser@rabbitmq01 ~/rpmbuild/SPECS $ rpmbuild -bb rabitmq-server-3.3.3.spec Once the command completes, the RPM will be written to the $HOME/rpmbuild/RPMS/noarch/rabbitmq-3.3.3-1.el6.noarch.rpm and can be installed onto any system. Things to note are: erlang is still a prerequisite to installing RabbitMQ. Please follow the instructions on RabbitMQ’s website for installing erlang. The custom RPM needs to be installed as the root user. The reason is because we’re creating directories in /opt which is owned by root, and we’re copying the rabbitmq-server init script to /etc/init.d. If these aren’t necessary for you, modify the spec file accordingly to the point where the RPM can be installed as mquser. To each their own, hence the flexibility being documented here in creating the RPM. Once complete, RabbitMQ can be started using the ‘/etc/init.d/rabbitmq-server start’ script as the mquser. I hope this helped!","categories":[{"name":"Rabbitmq","slug":"Rabbitmq","permalink":"http://kristianreese.com/categories/Rabbitmq/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"How to configure rsyslog 7.4.9 with TLS","slug":"How-to-configure-rsyslog-7-4-9-with-TLS","date":"2019-07-11T03:21:00.000Z","updated":"2019-07-10T22:32:38.000Z","comments":true,"path":"2019/07/11/How-to-configure-rsyslog-7-4-9-with-TLS/","link":"","permalink":"http://kristianreese.com/2019/07/11/How-to-configure-rsyslog-7-4-9-with-TLS/","excerpt":"","text":"How to configure rsyslog 7.4.9 with TLS If you’re looking to encrypt syslog transmissions between client and server, you can do so via rsyslog with TLS. Please see the external link to rsyslog.com below, in partiuclar the Overview section of the page describing encrypting syslog traffic with TLS (SSL). Also, if you’re an RHN subscriber, check out the RHN solution article. In looking at the links, you may notice both pertain to rsyslog versions &lt; 5.8.10, and the syntax for loading modules has changed in 7.4.9 and therefore examples provided on TLS configuration aren’t necessarily clear cut. In this article, I will show you how I combined these two external articles, and some insight garnered from rsyslog knowledgebase site kb.monitorware.com to work out the proper syntax for the latest stable release, rsyslog 7.4.9 (as of this writing). As of this writing, I actively have over 1800 clients actively connected to my rsyslog server: [root@rslserver ~]# netstat -an | grep &quot;:514 &quot; | wc -l 1865 SpecificationsBelow are the specifications I used: rsyslog certificate authority (CA) server: Red Hat Enterprise Linux 6 x86_64 with gnutls-utils-2.8.5-10.el6_4.2 package installed rsyslog server and rsyslog client: CentOS Linux 5.6 x86_64 with the following packages installed directly from the v7-stable rsyslog repository rsyslog-7.4.9-2.el5.centos rsyslog-gnutls-7.4.9-2.el5.centos How to configure rsyslog with TLSrsyslog CA server -&gt; linux01rsyslog server -&gt; rslserverrsyslog client -&gt; rslclient Setup the CA (Certificate Authority)The first thing that needs to be done is to setup the certificate authority, meaning we’ll be working with our RHEL6 server, linux01 to genereate a private key for the CA certificate and create a self signed certificate: Generate a private key for the CA certificate [root@linux01 ~]# certtool --generate-privkey --outfile ca-key.pem Generating a 2048 bit RSA private key... Generate a self-signed CA Certificate. I set a 10 year expiration period. [root@linux01 ~]# certtool --generate-self-signed --load-privkey ca-key.pem --outfile ca.pem Generating a self signed certificate... Please enter the details of the certificate&apos;s distinguished name. Just press enter to ignore a field. Country name (2 chars): US Organization name: kristianreese.com Organizational unit name: KB Locality name: STL State or province name: MO Common name: cacert UID: This field should not be used in new certificates. E-mail: Enter the certificate&apos;s serial number in decimal (default: 1395159808): Activation/Expiration time. The certificate will expire in (days): 3650 Extensions. Does the certificate belong to an authority? (y/N): y Path length constraint (decimal, -1 for no constraint): Is this a TLS web client certificate? (y/N): Is this also a TLS web server certificate? (y/N): Enter the e-mail of the subject of the certificate: kb@kristianreese.com Will the certificate be used to sign other certificates? (y/N): y Will the certificate be used to sign CRLs? (y/N): Will the certificate be used to sign code? (y/N): Will the certificate be used to sign OCSP requests? (y/N): Will the certificate be used for time stamping? (y/N): Enter the URI of the CRL distribution point: X.509 Certificate Information: [...] Is the above information ok? (Y/N): y Signing certificate... [root@linux01 ~]# ls -l total 136 -rw------- 1 root root 1675 Mar 18 11:12 ca-key.pem -rw-r--r-- 1 root root 1318 Mar 18 12:24 ca.pem [root@linux01 ~]# ca-key.pem is like the key to your car, or your house. You wouldn’t give those to a stranger, or leave them laying around in a public place where they may get stolen. Treat this file in the same regard. Nobody but the server on which it was generated needs to have it, so keep it permissioned as 600. Generate certificates for the rsyslog client and rsyslog server Generate a private key for the rsyslog client (rslclient) server: [root@linux01 ~]# certtool --generate-privkey --outfile rslclient-key.pem --bits 2048 Generating a 2048 bit RSA private key... Generate a certificate request for the rsyslog client (rslclient) server. Please be sure to enter the name of your system in the Common name field. The rsyslog authmode will be X509/name, meaning the certificate issued by the CA (linux01) is binding the public key to the Common name provided, and thus is the part that must match the host being authenticated. [root@linux01 ~]# certtool --generate-request --load-privkey rslclient-key.pem --outfile request.pem Generating a PKCS #10 certificate request... Country name (2 chars): US Organization name: kristianreese.com Organizational unit name: KB Locality name: STL State or province name: US Common name: webserver01.kristianreese.com UID: Enter a dnsName of the subject of the certificate: Enter the IP address of the subject of the certificate: Enter the e-mail of the subject of the certificate: Enter a challenge password: Does the certificate belong to an authority? (y/N): n Will the certificate be used for signing (DHE and RSA-EXPORT ciphersuites)? (y/N): Will the certificate be used for encryption (RSA ciphersuites)? (y/N): Is this a TLS web client certificate? (y/N): y Is this also a TLS web server certificate? (y/N): y [root@linux01 ~]# If you have multiple clients belonging to the same subdomain, you could use a wildcard in the Common name field to cover all of your clients with this one certificate. Simply input the Common name as *.example.com Generate the machine certificate using the request from step 2, and sign it with the Certifiate Authorities private key [root@linux01 ~]# certtool --generate-certificate --load-request request.pem --outfile rslclient-cert.pem --load-ca-certificate ca.pem --load-ca-privkey ca-key.pem Generating a signed certificate... Enter the certificate&apos;s serial number in decimal (default: 1395162401): Activation/Expiration time. The certificate will expire in (days): 3650 Extensions. Do you want to honour the extensions from the request? (y/N): Does the certificate belong to an authority? (y/N): n Is this a TLS web client certificate? (y/N): y Is this also a TLS web server certificate? (y/N): y Enter a dnsName of the subject of the certificate: webserver01.kristianreese.com Enter a dnsName of the subject of the certificate: Enter the IP address of the subject of the certificate: Will the certificate be used for signing (DHE and RSA-EXPORT ciphersuites)? (y/N): Will the certificate be used for encryption (RSA ciphersuites)? (y/N): X.509 Certificate Information: [...] Is the above information ok? (Y/N): y Signing certificate... [root@linux01 ~]# If you have multiple clients belonging to the same subdomain, you could use a wildcard in the dnsName field to cover all of your clients with one certificate. Simply input the dnsName as *.example.com (use your own domain name) The request.pem file is no longer needed and may be removed. Things should look as follows: [root@linux01 ~]# rm -f request.pem [root@linux01 ~]# ls -l total 144 -rw------- 1 root root 1675 Mar 18 11:12 ca-key.pem -rw-r--r-- 1 root root 1318 Mar 18 12:24 ca.pem -rw-r--r-- 1 root root 1424 Mar 18 13:07 rslclient-cert.pem -rw------- 1 root root 1675 Mar 18 12:42 rslclient-key.pem [root@linux01 ~]# Repeat steps 1 through 4, but this time for the rsyslog server. In summary: certtool --generate-privkey --outfile rslserver-key.pem --bits 2048 certtool --generate-request --load-privkey rslserver-key.pem --outfile request.pem Common name = FQDN of your rsyslog server -&gt; rslserver.kristianreese.com certtool --generate-certificate --load-request request.pem --outfile rslserver-cert.pem --load-ca-certificate ca.pem --load-ca-privkey ca-key.pem dnsName = FQDN of your rsyslog server -&gt; rslserver.kristianreese.com rm -f request.pem Your directory should contain something like: [root@linux01 ~]# ls -l total 148 -rw------- 1 root root 1675 Mar 18 11:12 ca-key.pem -rw-r--r-- 1 root root 1318 Mar 18 12:24 ca.pem &lt;/span&gt;&lt;span style=&quot;color: rgb(0, 255, 0);&quot;&gt;&lt;span style=&quot;color: rgb(0, 255, 0);&quot;&gt;-rw-r--r-- 1 root root 1424 Mar 18 13:07 rslclient-cert.pem -rw------- 1 root root 1675 Mar 18 12:42 rslclient-key.pem&lt;/span&gt; -rw-r--r-- 1 root root 1415 Mar 18 13:42 rslserver-cert.pem -rw------- 1 root root 1675 Mar 18 13:40 rslserver-key.pem [root@linux01 ~]# This completes the setup of the certificates. With the exception of ca-key.pem, these certificates need to be copied to their respective systems before we can continue with configuring rsyslog to use them. Remember to keep ca-key.pem safe! Nobody should have a copy of this file except the CA. In addition, each client-key should only be copied to the machine for which it was generated, except in the case of wildcard certificates in which case only those within the identified subdomain should receive a copy. Copy certificates from CA to rsyslog server and rsyslog client Where do the files get copied? When the rsyslog rpm is installed, it creates a directory /etc/pki/rsyslog/. This is a good place to copy the files to: Simply scp a copy of ca.pem, rslclient-cert.pem, and rslclient-key.pem to your rsyslog client system(s) /etc/pki/rsyslog/. Simple scp a copy of ca.pem, rslserver-cert.pem, and rslserver-key.pem to your rsyslog server (the collector) /etc/pki/rsyslog/. It’s worth double checking that the certificate files ownerships are set to root:root and 0600.~# chmod 0600 /etc/pki/rsyslog/* Configure rsyslog server After installing the two rsyslog packages listed in the specifications section above (ie): [root@rslserver ~]# rpm -qa | grep rsyslog rsyslog-7.4.9-2.el5.centos rsyslog-gnutls-7.4.9-2.el5.centos you are ready to configure rsyslog. Begin by editing /etc/rsyslog.conf and adding the following just after the defined MODULES section: # Increase the amount of open files rsyslog is allowed, which includes open tcp sockets # This is important if there are many clients. http://www.rsyslog.com/doc/rsconf1_maxopenfiles.html $MaxOpenFiles 2048 # make gtls driver the default $DefaultNetstreamDriver gtls # certificate files generated on RHEL6 and stored in /root $DefaultNetstreamDriverCAFile /etc/pki/rsyslog/ca.pem $DefaultNetstreamDriverCertFile /etc/pki/rsyslog/rslserver-cert.pem $DefaultNetstreamDriverKeyFile /etc/pki/rsyslog/rslserver-key.pem # Provides TCP syslog reception # for parameters see http://www.rsyslog.com/doc/imtcp.html module(load=&quot;imtcp&quot; MaxSessions=&quot;2000&quot; StreamDriver.mode=&quot;1&quot; StreamDriver.authmode=&quot;x509/name&quot; PermittedPeer=&quot;*.kristianreese.com&quot;) input(type=&quot;imtcp&quot; port=&quot;514&quot; name=&quot;tcp-tls&quot;) For any concerns over the use of wildcard SSL certificates, consider using the $AllowedSender directive option available to the rsyslog configuration. restart rsyslog: /etc/init.d/rsyslog restart Verify /var/log/messages did not log an error and that rsyslog is running with ps -ef | grep rsyslog Configure rsyslog client Again, after installing the same rsyslog packages as the rsyslog server, you are ready to configure rsyslog. For each client, instead of modifying /etc/rsyslog.conf, I created an include file within the /etc/rsyslog.d directory: [root@0rslclient ~]# cat /etc/rsyslog.d/tls.conf # make gtls driver the default $DefaultNetstreamDriver gtls # certificate files $DefaultNetstreamDriverCAFile /etc/pki/rsyslog/ca.pem $DefaultNetstreamDriverCertFile /etc/pki/rsyslog/rslclient-cert.pem $DefaultNetstreamDriverKeyFile /etc/pki/rsyslog/rslclient-key.pem #### GLOBAL DIRECTIVES #### # Use default timestamp format $ActionFileDefaultTemplate RSYSLOG_TraditionalFileFormat # gtls Network Stream Driver # x509/name - certificate validation and subject name authentication # http://www.rsyslog.com/doc/ns_gtls.html $ActionSendStreamDriverAuthMode x509/name $ActionSendStreamDriverPermittedPeer rslserver.kristianreese.com $ActionSendStreamDriverMode 1 # run driver in TLS-only mode :msg, contains, &quot;Failed password&quot; authpriv.* @@rslserver.kristianreese.com:514 [root@rslclient ~]# In this example, I am using a property-based filter (:msg) to capture only syslog messages that contain “Failed password” as commonly found in /var/log/secure during invalid login attempts. The @@ is intentional and tells rsyslog we’re wanting to send this message to a remote host. restart rsyslog: /etc/init.d/rsyslog restart Verify /var/log/messages did not log an error and that rsyslog is running with ps -ef | grep rsyslog ValidationThe configuratio is now ready for validation. Login to the rslserver, and begin tailing /var/log/secure with the command: ~# tail -f /var/log/secure Using another session, test by ssh’ing into the rslclient, and enter invalid credentials (remember we’re only capturing messages with the string “Failed password”). After the failed login attempts, the messages should display from the tail output on the rslserver. If you want to log remote messages into their own log file, you can use filters… something like: ...# The authpriv file has restricted access. :hostname, isequal, &quot;rslserver&quot; authpriv.* /var/log/secure :msg, contains, &quot;Failed password&quot; authpriv.* /var/log/secure.remote ... This setup logs anything destined for authpriv from the rslserver to its normal log /var/log/secure, and to /var/log/secure.remote. If you want messages to stop processing after an action, then read up on rsyslog.com about the stop directive. If the message received is from an allowed client, and contains the phrase “Failed password”, it is only logged to /var/log/secure.remote. Verification:From the client machine, capture packets for examination of the before TLS and after TLS config to see for youself that the traffic is being encrypted. This can be used for importation into wireshark, for example. rslclient ~# tcpdump -nnvvvS -s 0 -U -w /tmp/sniff.rsyslog dst &lt;ip of rslserver&gt; and dst port 514 Otherwise, from command line, you can simply observe the traffic: BeforeTo listen/watch/verify for encrypted traffic, run tcpdump on the rslserver:~# tcpdump -nn -s2048 -X host rslclient.domain.com Here are some screenshots of a test I ran before configuring TLS to show that the traffic is in fact being encrypted after following the above steps: AFTERafter TLS has been configured, run the tcpdump command again with a different file name: ~# tcpdump -nnvvvS -s 0 -U -w /tmp/sniff.myVPS.rsyslog dst &lt;rslserver IP&gt; and dst port 514 END","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"How to force NFSv4 via config files instead of mount options","slug":"How-to-force-NFSv4-via-config-files-instead-of-mount-options","date":"2019-07-11T03:16:00.000Z","updated":"2019-07-10T22:17:30.000Z","comments":true,"path":"2019/07/11/How-to-force-NFSv4-via-config-files-instead-of-mount-options/","link":"","permalink":"http://kristianreese.com/2019/07/11/How-to-force-NFSv4-via-config-files-instead-of-mount-options/","excerpt":"","text":"As a whole, Linux can be configured to enforce all NFS mounts to use version 3 instead of defaulting to version 4. While the easiest solution is to specify nfsvers=3 or vers=3 as a mount option to fstab entries or within an automount config file, there are other options available. /etc/nfsmount.confChange Defaultvers=4 to Defaultvers=3 and uncomment out the entry in /etc/nfsmount.conf: [ NFSMount_Global_Options ]# This statically named section defines global mount# options that can be applied on all NFS mount.## Protocol Version [2,3,4]# This defines the default protocol version which will# be used to start the negotiation with the server.# Defaultvers=4Defaultvers=3 /etc/sysconfig/autofsChange OPTIONS=&quot;&quot; to OPTIONS=&quot;-O vers=3&quot; in /etc/sysconfig/autofs: (option is towards bottom of config file) OPTIONS=&quot;-O vers=3&quot; Restart autofs:/etc/init.d/autofs restart END","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Redirect command line results into while loop","slug":"Redirect-command-line-results-into-while-loop","date":"2019-07-11T03:09:00.000Z","updated":"2019-07-10T22:10:47.000Z","comments":true,"path":"2019/07/11/Redirect-command-line-results-into-while-loop/","link":"","permalink":"http://kristianreese.com/2019/07/11/Redirect-command-line-results-into-while-loop/","excerpt":"","text":"In a scenario where there are various config files contained throughout various subdirectories where one needs to commit mass updates quickly, it’s possible to redirect the results of a command into a while loop. #!/bin/bashcd /opt/Jboss_5.1.2/server/for file in file1.xml file2.xml file3.xml file4.xml file5.xmldo while read filename do #sed -i -e 's:&lt;old_password&gt;:&lt;new_password&gt;:g' $filename ls -1 $&#123;filename&#125; done &lt; &lt;(find . -name $&#123;file&#125;)done RESULTS [kreese@linux01 /]$ ~/scripts/updatepw2.sh ./JbossServer1/deploy/file1.xml./JbossServer2/deploy/file1.xml./JbossServer3/deploy/file1.xml./JbossServer1/deploy/file2.xml./JbossServer2/deploy/file2.xml./JbossServer3/deploy/file2.xml./JbossServer1/deploy/file3.xml./JbossServer2/deploy/file3.xml./JbossServer3/deploy/file3.xml./JbossServer1/deploy/file4.xml./JbossServer2/deploy/file4.xml./JbossServer3/deploy/file4.xml./JbossServer1/deploy/file5.xml./JbossServer2/deploy/file5.xml./JbossServer3/deploy/file5.xml END","categories":[{"name":"Scripting","slug":"Scripting","permalink":"http://kristianreese.com/categories/Scripting/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Daily Log Maintenance","slug":"Daily-Log-Maintenance","date":"2019-07-11T03:01:00.000Z","updated":"2019-07-10T22:09:38.000Z","comments":true,"path":"2019/07/11/Daily-Log-Maintenance/","link":"","permalink":"http://kristianreese.com/2019/07/11/Daily-Log-Maintenance/","excerpt":"","text":"For systems that have lots of application logs, this shell script can be run from cron on a daily basis to maximize space utiliazation by gzipping and deleting old log files by the defined time limitations #!/bin/sh # # app/logs/current log maintenance. by: jhazelwo, 2012 # # meant to be run daily from cron # #30 0 * * * /app/bin/logMaint.sh &gt;&gt; /var/log/cron.err 2&gt;&amp;1 # matches=&quot;server.log_*[0-9] server_20*.log server_access_log*[0-9] server_access_log*[0-9]s business.log.*[0-9] error.log.*[0-9] info.log.*[0-9] other.log.*[0-9] sf.log.*[0-9] warn.log.*[0-9] other.log.20*[0-9] error.log.20*[0-9] warn.log.20*[0-9] info.log.20*[0-9] business.log.20*[0-9] server.log_20*[0-9] sf.log.20*[0-9]&quot;&lt;/span&gt; &lt;span style=&quot;font-family: Courier New;&quot;&gt; Where=&quot;/data/applications/log&quot; gzipLimit=&quot;-mtime +30&quot; rmLimit=&quot;-mtime +120&quot; cd / thisExec=`basename $0` thisProc=&quot;$&#123;thisExec&#125;[$]&quot; thisHost=&quot;`hostname | head -1 | cut -d\\. -f1`&quot; DEBUG=0 for Here in $&#123;Where&#125;; do for name in $&#123;matches&#125;; do echo &quot;`date` $&#123;thisHost&#125; $&#123;thisProc&#125; gzip $&#123;name&#125; in $&#123;Here&#125;&quot; if [ $&#123;DEBUG&#125; -eq 1 ]; then echo /usr/bin/find $&#123;Here&#125; -mount -type f $&#123;gzipLimit&#125; -name &quot;$&#123;name&#125;&quot; -exec ls -al &#123;&#125; \\; /usr/bin/find $&#123;Here&#125; -mount -type f $&#123;gzipLimit&#125; -name &quot;$&#123;name&#125;&quot; -exec ls -al &#123;&#125; \\; else /usr/bin/find $&#123;Here&#125; -mount -type f $&#123;gzipLimit&#125; -name $&#123;name&#125; -exec /bin/gzip -9vf &#123;&#125; \\; fi done for name in $&#123;matches&#125;; do echo &quot;`date` $&#123;thisHost&#125; $&#123;thisProc&#125; rm $&#123;name&#125;.gz in $&#123;Here&#125;&quot; if [ $&#123;DEBUG&#125; -eq 1 ]; then echo /usr/bin/find $&#123;Here&#125; -mount -type f $&#123;rmLimit&#125; -name $&#123;name&#125;.gz -exec ls -al &#123;&#125; \\; -print /usr/bin/find $&#123;Here&#125; -mount -type f $&#123;rmLimit&#125; -name $&#123;name&#125;.gz -exec ls -al &#123;&#125; \\; -print else /usr/bin/find $&#123;Here&#125; -mount -type f $&#123;rmLimit&#125; -name $&#123;name&#125;.gz -exec rm -f &#123;&#125; \\; -print fi done done","categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"curl with cookie authentication","slug":"curl-with-cookie-authentication","date":"2019-07-07T04:04:00.000Z","updated":"2019-07-06T18:05:04.000Z","comments":true,"path":"2019/07/07/curl-with-cookie-authentication/","link":"","permalink":"http://kristianreese.com/2019/07/07/curl-with-cookie-authentication/","excerpt":"","text":"How to download files from sites requiring authenticationHave you ever logged into a website that requires you login before you can download a file? Subsequently, a simple ‘copy-link’ option does not allow you to wget the file and a HTTP 403 Authorization Failed message is thrown. In order to get around this, I’ve used this method when a cookie for authentication is used: If not installed, download and install Chrome Download and install cookie.txt export from the Chrome Web Store Using Chrome, login to the site where you’re attempting to wget or curl a file for download. Click on the cookie.txt export button next to the address bar in Chrome, and copy the contents into a file, such as cookies.txt ~# curl -L -b cookies.txt http://authreq.com/todownload/thisfile.iso -o name.iso -L follow redirects; -b use the cookie in specified file; -o write ouput to this local file Comments are welcome on alternative ways using other web browsers.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"curl","slug":"curl","permalink":"http://kristianreese.com/tags/curl/"}],"author":"Kris Reese"},{"title":"concatenate a file list into a string","slug":"concatenate-a-file-list-into-a-string","date":"2019-07-06T23:28:00.000Z","updated":"2019-07-06T18:29:59.000Z","comments":true,"path":"2019/07/06/concatenate-a-file-list-into-a-string/","link":"","permalink":"http://kristianreese.com/2019/07/06/concatenate-a-file-list-into-a-string/","excerpt":"","text":"Example 1kreese@myvps:arp $ 205.178.186.72 205.178.136.60 205.178.136.72 205.178.136.91 205.178.136.109 205.178.136.132 205.178.136.166 205.178.186.206 205.178.186.220 206.188.212.32 206.188.212.142 206.188.212.148 206.188.212.163 206.188.212.164 206.188.212.165 206.188.212.166 206.188.212.167 206.188.212.168 206.188.212.169 206.188.212.170 206.188.212.171 206.188.212.193 69.161.223.12 69.161.223.104 69.161.223.236 69.161.223.237 206.188.208.186 206.188.207.1 206.188.207.209 206.188.207.254 206.188.206.50 206.188.206.60 206.188.206.178 206.188.206.180 206.188.204.129 206.188.204.254 kreese@myvps:arp $ kreese@myvps:arp $ ips=$(&lt; ./ips awk &apos;$1&apos;)kreese@myvps:arp $ echo $ips205.178.186.72 205.178.136.60 205.178.136.72 205.178.136.91 205.178.136.109 205.178.136.132 205.178.136.166 205.178.186.206 205.178.186.220 206.188.212.32 206.188.212.142 206.188.212.148 206.188.212.163 206.188.212.164 206.188.212.165 206.188.212.166 206.188.212.167 206.188.212.168 206.188.212.169 206.188.212.170 206.188.212.171 206.188.212.193 69.161.223.12 69.161.223.104 69.161.223.236 69.161.223.237 206.188.208.186 206.188.207.1 206.188.207.209 206.188.207.254 206.188.206.50 206.188.206.60 206.188.206.178 206.188.206.180 206.188.204.129 206.188.204.254 Example 2:nodevs=$(&lt; /proc/filesystems awk ‘$1 == “nodev” { print $2 }’) Example 3:One of my favorites is when I need to kill off a bunch of processes belonging to a specific user. In comes in really handy when a customers VPS is hacked and they’ve created a privlidged user account where you want to quickly kill of a bunch of pids belonging to that user: kill -9 $(ps -U 10000 | awk &#39;{ print $1 }&#39; | grep -v PID) or use ps -U apache -o pid --no-headers Example 4:Using tr tr -s &#39;\\n&#39; &#39; &#39; &lt; file.txt Example 5:[root@vps11 ~]# for veid in $(cat vps5.migrations | awk &apos;&#123; print $4 &#125;&apos;); do echo -ne &quot;$veid,&quot;; done 7879,7868,11370,7843,8493,12116,13739,7915,7954,8640,[root@vps11 ~]#","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"scripting","slug":"scripting","permalink":"http://kristianreese.com/tags/scripting/"}],"author":"Kris Reese"},{"title":"ternary example","slug":"ternary-example","date":"2019-07-06T23:26:00.000Z","updated":"2019-07-06T18:27:27.000Z","comments":true,"path":"2019/07/06/ternary-example/","link":"","permalink":"http://kristianreese.com/2019/07/06/ternary-example/","excerpt":"","text":"Just a quick little example on ternary bash syntax. [[ $member = *mongodbarb* ]] &amp;&amp; port=30000 || port=27017 Examplereese@MacBook ~ $ node=webserver01 reese@MacBook ~ $ [[ $node = *webserver* ]] && port=30000 || port=27017 reese@MacBook ~ $ echo $port 30000 reese@MacBook ~ $ node=appserver01 reese@MacBook ~ $ [[ $node = *webserver* ]] && port=30000 || port=27017 reese@MacBook ~ $ echo $port 27017","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"bash","slug":"bash","permalink":"http://kristianreese.com/tags/bash/"},{"name":"scripting","slug":"scripting","permalink":"http://kristianreese.com/tags/scripting/"}],"author":"Kris Reese"},{"title":"couldn't find device with uuid","slug":"couldn-t-find-device-with-uuid","date":"2019-07-06T23:17:00.000Z","updated":"2019-07-06T18:25:33.000Z","comments":true,"path":"2019/07/06/couldn-t-find-device-with-uuid/","link":"","permalink":"http://kristianreese.com/2019/07/06/couldn-t-find-device-with-uuid/","excerpt":"","text":"Linux LVM commands result in Couldn’t find device with uuid Couldn’t find all physical volumes for volume group Help! Commands like lvs, lvdisplay, vgdisplay, and pvscan result in an error like the following: Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg.Volume group &quot;metabackupvg&quot; not found # pvscanCouldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;.Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;.Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;.Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. PV /dev/emcpoweraa1 VG srcdata1vg lvm2 [50.00 GB / 0 free] PV /dev/emcpowerz1 VG srcdata2vg lvm2 [50.00 GB / 0 free] PV /dev/emcpowerj1 VG metalogvg lvm2 [3.00 GB / 0 free] PV /dev/emcpowerl1 VG metalogvg lvm2 [3.00 GB / 0 free] PV /dev/emcpowern1 VG metalogvg lvm2 [4.99 GB / 0 free] PV /dev/emcpowerad1 VG metalogvg lvm2 [14.00 GB / 0 free] PV /dev/emcpowery1 VG auditbackupvg lvm2 [229.99 GB / 0 free] PV /dev/emcpoweru1 VG metabackupvg lvm2 [45.00 GB / 0 free] PV unknown device VG metabackupvg lvm2 [69.99 GB / 0 free] PV /dev/emcpoweraf1 VG metabackupvg lvm2 [124.99 GB / 0 free] PV /dev/emcpowerv1 VG auditlogvg lvm2 [3.00 GB / 0 free] PV /dev/emcpowerw1 VG auditlogvg lvm2 [3.00 GB / 0 free] PV /dev/emcpowerx1 VG auditlogvg lvm2 [3.00 GB / 0 free] PV /dev/emcpowert1 VG auditlogvg lvm2 [89.99 GB / 0 free] PV /dev/emcpowerb1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerc1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerd1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowere1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerk1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerm1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowero1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerq1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerp1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerr1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerab1 VG auditdatavg lvm2 [25.00 GB / 0 free] PV /dev/emcpowerf1 VG metadatavg lvm2 [11.00 GB / 0 free] PV /dev/emcpowerg1 VG metadatavg lvm2 [11.00 GB / 0 free] PV /dev/emcpowerh1 VG metadatavg lvm2 [11.00 GB / 0 free] PV /dev/emcpoweri1 VG metadatavg lvm2 [11.00 GB / 0 free] PV /dev/emcpowers1 VG metadatavg lvm2 [20.00 GB / 0 free] PV /dev/emcpowera1 VG metadatavg lvm2 [20.00 GB / 0 free] PV /dev/emcpowerae1 VG metadatavg lvm2 [14.00 GB / 0 free] Total: 32 [1.04 TB] / in use: 32 [1.04 TB] / in no VG: 0 [0 ] Solution This scenario indicates that a disk is missing from the volume group. This is clear since pvscan scans all disks for physical volumes, and it’s telling us that it cannot find a device. In my case, I deleted a LUN from the EMC that I knew was no longer being used, but I forgot to update the LVM configuration. To correct the issue, I removed the lost physical volume from the volume group using the --removemissing argument of the vgreduce command. Tip: Run vgreduce with the --test and --verbose options to validate what will be executed and confirm this is what you want to do. [root@dwetlprod2 ~]# vgreduce --removemissing --verbose metabackupvg Finding volume group &quot;metabackupvg&quot; Wiping cache of LVM-capable devices Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find all physical volumes for volume group metabackupvg. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Couldn&apos;t find device with uuid &apos;R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP&apos;. Archiving volume group &quot;metabackupvg&quot; metadata (seqno 73). metabackupvg/backup2lv has missing extents: removing (including dependencies) Removing LV backup2lv from VG metabackupvg Removing PV with UUID R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP from VG metabackupvg Creating volume group backup &quot;/etc/lvm/backup/metabackupvg&quot; (seqno 74). Wrote out consistent volume group metabackupvg [root@dwetlprod2 ~]# [root@dwetlprod2 ~]# pvcreate --uuid R5zG2s-g4yi-ytEO-8Xvk-2B0s-S36G-9nWhXP /dev/emcpowerah Physical volume &quot;/dev/emcpowerah&quot; successfully created [root@dwetlprod2 ~]# vgcfgrestore metabackupvg Restored volume group metabackupvg [root@dwetlprod2 ~]# vgscan Reading all physical volumes. This may take a while... Found volume group &quot;srcdata1vg&quot; using metadata type lvm2 Found volume group &quot;srcdata2vg&quot; using metadata type lvm2 Found volume group &quot;metalogvg&quot; using metadata type lvm2 Found volume group &quot;auditbackupvg&quot; using metadata type lvm2 Found volume group &quot;auditlogvg&quot; using metadata type lvm2 Found volume group &quot;metabackupvg&quot; using metadata type lvm2 Found volume group &quot;auditdatavg&quot; using metadata type lvm2 Found volume group &quot;metadatavg&quot; using metadata type lvm2 Run a file system check on the device then mount it. If your device (LUN) was mistakenly deleted, or in the case of physical media the disk has gone kaput, then you’ll need to restore data from backups. You may have noticed that I didn’t even fdisk the replacement device. Don’t forget to do this step if you will be restoring data and you’re restoring to a partition.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"lvm","slug":"lvm","permalink":"http://kristianreese.com/tags/lvm/"}],"author":"Kris Reese"},{"title":"expand alias in shell script","slug":"expand-alias-in-shell-script","date":"2019-07-06T23:15:00.000Z","updated":"2019-07-06T18:16:54.000Z","comments":true,"path":"2019/07/06/expand-alias-in-shell-script/","link":"","permalink":"http://kristianreese.com/2019/07/06/expand-alias-in-shell-script/","excerpt":"","text":"Defining an alias within a shell scriptIf there’s a need to define an alias within a shell script, shopt needs to be defined to tell the script to expand the alias. This option is enabled by default for interactive shells, but not for non-interactive shells hence the need to use the shopt builtin. For more information, read the manpage on shopt. Here is a working example: #!/bin/bash shopt -s expand_aliases rel=`cat /etc/virtuozzo-release | awk '&#123; print $3 &#125;'` if [ \"$rel\" = \"4.6.0\" ]; then FS=1 else FS=0 fi if [ \"$1\" = \"debug\" ]; then alias echo='echo -ne' cnt=1 vzlist -aHh 0*.netsolvps.com -o veid &gt; veids.txt #/bin/cat -n ./veids.txt else cnt='' fi for veid in `vzlist -aHh 0*.netsolvps.com -o veid` do echo \"$cnt \" if [ $FS -eq 1 ]; then KEYPATH=\"/vz/private/$veid/fs\" /bin/grep key-number $KEYPATH/root/etc/sw/keys/keys/* else KEYPATH=\"/vz/private/$veid\" if [ -d $KEYPATH/root/etc/sw/keys/keys ]; then /bin/grep key-number $KEYPATH/root/etc/sw/keys/keys/* else /bin/grep PLSK $KEYPATH/root/etc/psa/psa.key fi fi if [ ! -z \"$cnt\" ]; then cnt=`expr $cnt + 1` fi done One way around this is to use the eval command instead, whereby a user can assign a command to a variable and execute the variable as a command. In this example, the variable debug is assigned the value of “echo -n”, which is the command to suppress newline character from the default behavior of the echo command. This way, I won’t have to check any flags should I want to DEBUG the output of the results. the option DEBUG would have to be passed to the script name as the first argument in the command line to suppress the newline character as it outputs the value of veid. Otherwise, if DEBUG is omitted, then a “silent” echo is output to the screen which goes unnoticed since the newline character has been surpressed. #!/bin/bash debug=&quot;echo -n&quot; if [ &quot;$1&quot; == DEBUG ]; then debug=&quot;echo -n \\$&#123;veid&#125;&apos;,&apos;&quot; fi for veid in $(vzlist -1h 0*.netsolvps.com) do #echo CTID depending on whether or not DEBUG was passed as option to command **&lt;span style=&quot;color: rgb(255, 0, 0);&quot;&gt;eval $debug&lt;/span&gt;** if [ -d /vz/private/$veid/fs ]; then KEYPATH=&quot;/vz/private/$veid/fs&quot; key=`/bin/egrep -i &quot;key-number|:domains &quot; $KEYPATH/root/etc/sw/keys/keys/*` else KEYPATH=&quot;/vz/private/$veid&quot; if [ -d $KEYPATH/root/etc/sw/keys/keys ]; then key=`/bin/egrep -i &quot;key-number|:domains &quot; $KEYPATH/root/etc/sw/keys/keys/*` else key=`/bin/egrep -i &quot;Key number: |Number of Domains:&quot; $KEYPATH/root/etc/psa/psa.key` fi fi echo $key | grep -i key done","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"bash","slug":"bash","permalink":"http://kristianreese.com/tags/bash/"},{"name":"scripting","slug":"scripting","permalink":"http://kristianreese.com/tags/scripting/"}],"author":"Kris Reese"},{"title":"Rename batch of files","slug":"Rename-batch-of-files","date":"2019-07-06T23:13:00.000Z","updated":"2019-07-06T18:13:57.000Z","comments":true,"path":"2019/07/06/Rename-batch-of-files/","link":"","permalink":"http://kristianreese.com/2019/07/06/Rename-batch-of-files/","excerpt":"","text":"How to rename a batch or group of fileskreese@MacBook ~/wbsvr $ ls -l total 200 -rwx------ 1 kreese 1575811233 34652 Mar 19 16:49 httpd.conf -rw-r--r-- 1 kreese 1575811233 13609 Mar 25 08:30 vhost1.conf.txt -rw-r--r-- 1 kreese 1575811233 21861 Mar 25 08:30 vhost2.conf.txt -rw-r--r-- 1 kreese 1575811233 21883 Mar 25 08:30 vhost3.conf.txt kreese@MacBook ~/wbsvr $ kreese@MacBook ~/wbsvr $ for file in *.txt; do x=$(basename $file .txt); mv $file $x; done kreese@MacBook ~/wbsvr $ kreese@MacBook ~/wbsvr $ ls -l total 200 -rwx------ 1 kreese 1575811233 34652 Mar 19 16:49 httpd.conf -rw-r--r-- 1 kreese 1575811233 13609 Mar 25 08:30 vhost1.conf -rw-r--r-- 1 kreese 1575811233 21861 Mar 25 08:30 vhost2.conf -rw-r--r-- 1 kreese 1575811233 21883 Mar 25 08:30 vhost3.conf","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"bash","slug":"bash","permalink":"http://kristianreese.com/tags/bash/"}],"author":"Kris Reese"},{"title":"Querying Data with PL/SQL: Implicit Cursor FOR Loop","slug":"Querying-Data-with-PL-SQL-Implicit-Cursor-FOR-Loop","date":"2019-06-09T04:25:00.000Z","updated":"2019-06-08T18:31:16.000Z","comments":true,"path":"2019/06/09/Querying-Data-with-PL-SQL-Implicit-Cursor-FOR-Loop/","link":"","permalink":"http://kristianreese.com/2019/06/09/Querying-Data-with-PL-SQL-Implicit-Cursor-FOR-Loop/","excerpt":"","text":"Looping through result sets from SELECT statement using PL/SQLOne day, I was looking to update a bunch of records with a stored procedure via a for loop using PL/SQL. In order to accomplish this, I used an Implicit Cursor FOR Loop. Here is my example: BEGIN FOR item IN (select cust_id from products where server_id = 1542 and cust_status in (10,11)) LOOP PROD_PKG_V1.migrateCustInDb ( item.cust_id , 2783 ); END LOOP; END; PL/SQL procedure successfully completed. If the desire is to display the output before committing execution of the stored procedure, DBMS_OUTPUT.PUT_LINE can be used. You may have to SET SERVEROUTPUT ON FORMAT WRAPPED from the SQL prompt for DBMS_OUTPUT.PUT_LINE to actually display output to the console. In the example below, I used ed to edit the previous example. In order for ed to work, ensure that export EDITOR=vi is set it your oracle profile ed ~ ~ &quot;afiedt.buf&quot; 8L, 189C written 1 BEGIN 2 FOR item IN 3 (select cust_id from products where server_id = 2783 and cust_status in (10,11)) 4 LOOP 5 DBMS_OUTPUT.PUT_LINE(&apos;cust_id = &apos; || item.cust_id); 6 END LOOP; 7* END; SQL&gt;&lt;/span&gt; &lt;span style=&quot;font-family: Courier New;&quot;&gt;&lt;span style=&quot;font-family: Courier New;&quot;&gt;SET SERVEROUTPUT ON FORMAT WRAPPED SQL&gt; / cust_id = WN.HP.251736914 cust_id = WN.HP.252888120 cust_id = WN.HP.255425860 cust_id = WN.HP.255721892 cust_id = WN.HP.255919690 PL/SQL procedure successfully completed. SQL&gt; Or to output the SQL / stored procedure statement for copy/paste into a shell script or what-have-you: DECLARE updpkgsql VARCHAR(1000); BEGIN FOR item IN (select cust_id from products where server_id = 1542 and cust_status in (10,11)) LOOP updpkgsql:=&apos;exec&lt;/span&gt; &lt;span style=&quot;font-family: Courier New;&quot;&gt;&lt;span style=&quot;font-family: Courier New;&quot;&gt;PROD_PKG_V1.migrateCustInDb&lt;/span&gt; ( &apos; || item.cust_id || &apos;, 2783 );&apos;; DBMS_OUTPUT.PUT_LINE(&apos;sql = &apos; || updpkgsql); END LOOP; END; sql = exec PROD_PKG_V1.migrateCustInDb ( WN.HP.251736914, 2783 ); sql = exec PROD_PKG_V1.migrateCustInDb ( WN.HP.252888120, 2783 ); sql = exec PROD_PKG_V1.migrateCustInDb ( WN.HP.255425860, 2783 ); sql = exec PROD_PKG_V1.migrateCustInDb ( WN.HP.255721892, 2783 ); sql = exec PROD_PKG_V1.migrateCustInDb ( WN.HP.255919690, 2783 ); PL/SQL procedure successfully completed. SQL&gt;","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://kristianreese.com/tags/SQL/"}],"author":"Kris Reese"},{"title":"Various Puppet Errors","slug":"Various-Puppet-Errors","date":"2019-06-08T23:08:00.000Z","updated":"2019-06-08T18:15:19.000Z","comments":true,"path":"2019/06/08/Various-Puppet-Errors/","link":"","permalink":"http://kristianreese.com/2019/06/08/Various-Puppet-Errors/","excerpt":"","text":"Error: Could not request certificate: SSL_connect returned=1 errno=0 state=error: certificate verify failed: [CRL is not yet valid for /CN=Puppet CA generated on pemaster at 2015-03-17 12:14:32 -0500] The time between the puppet agent node and the puppet master is out of sync. Sync the time, and regenerate the cert. Reference Evaluation Error: Operator ‘[]’ is not applicable to an Undef Value If using facts to evaluate inside of a condition, you won’t be able to access secondary elements if one isn’t returned to the first element. # If false, include rhel::resolv unless $::facts['dhcp_servers']['system'] &#123; include rhel::resolv &#125; If this systems interface was not configured via DHCP, a puppet run would result in: Error: Could not retrieve catalog from remote server: Error 400 on SERVER: Evaluation Error: Error while evaluating a Resource Statement, Evaluation Error: Operator &apos;[]&apos; is not applicable to an Undef Value. at /etc/puppetlabs/code/environments/snd/modules/rhel/manifests/init.pp:25:10 on node mongodb This means that the facts[&#39;dhcp_servers&#39;] is empty, which results in an undef value, as documented by Puppet: “whenever no other value is applicable, the value is undef” Thus, we cannot access the [‘system’] element, just as the error suggests: Operator &#39;[]&#39; is not applicable to an Undef Value. Since the dhcp_server fact will result in an undef value provided the servers interface was not setup via DHCP, we can simply rely on it to satisfy our conditional which aims whether or not to include rhel::resolv.","categories":[{"name":"Puppet","slug":"Puppet","permalink":"http://kristianreese.com/categories/Puppet/"}],"tags":[{"name":"Puppet","slug":"Puppet","permalink":"http://kristianreese.com/tags/Puppet/"}],"author":"Kris Reese"},{"title":"Using puppet-classify to interface with the classifier service","slug":"Using-puppet-classify-to-interface-with-the-classifier-service","date":"2019-06-08T22:09:00.000Z","updated":"2019-06-08T17:31:11.000Z","comments":true,"path":"2019/06/08/Using-puppet-classify-to-interface-with-the-classifier-service/","link":"","permalink":"http://kristianreese.com/2019/06/08/Using-puppet-classify-to-interface-with-the-classifier-service/","excerpt":"","text":"I’ve been looking for an opportunity to share something with the Puppet Community and I think I found it in puppet-classify and how I’m using it along with MongoDB to manage classifications in a Puppet Enterprise deployment. The approach I’ve taken is to backup and store our Puppet Classifications into MongoDB. That way, in the event of a rebuild, we can easily reimport all of our classifications. Where this approach has additional value is in tear down environment such as Vagrant. Puppet module developers can use Vagrant to deploy their own local puppetmaster and tear it down/rebuild it as often as they like since they can store their classifications in MongoDB and import them with ease. Anytime a developer wants to save off a classification, they can easily export to MongoDB for later retrieval or sharing with a co-worker. Check it out on my github. This is my first ruby script","categories":[{"name":"Puppet","slug":"Puppet","permalink":"http://kristianreese.com/categories/Puppet/"}],"tags":[{"name":"Puppet","slug":"Puppet","permalink":"http://kristianreese.com/tags/Puppet/"}],"author":"Kris Reese"},{"title":"Home Lab DNS Using dnsmasq and Puppet","slug":"Home-Lab-DNS-Using-dnsmasq-and-Puppet","date":"2019-05-06T12:19:00.000Z","updated":"2019-05-06T02:36:58.000Z","comments":true,"path":"2019/05/06/Home-Lab-DNS-Using-dnsmasq-and-Puppet/","link":"","permalink":"http://kristianreese.com/2019/05/06/Home-Lab-DNS-Using-dnsmasq-and-Puppet/","excerpt":"","text":"It has been a long while since I’ve published anything to my kb site, perhaps just as long since I’ve started a new job nearly 2.5 years ago. In that time, my job responsibilities have shifted from the things I used to write on often. It’s my hope that I’ll get back into contributing to my kb site as I’ve enjoyed the collaboration with those of you who visit. So let’s get to it! I’m going to try something new this time around. Here is a video overview on the installation and setup of DNS. (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: \"ca-pub-8101539885827084\", enable_page_level_ads: true }); Followed by a write up on the overall approach I’ve taken to setup DNS for my home network. First, let’s outline the article: dnsmasq installing and configuration dnsmasq for DNS services only Putting it to use Is the DNS service to be used by lab systems only? Is the DNS service to be used for the entire home network? Inputting and managing DNS entries Puppet How Puppet can be used to automate the management of DNS entries How Puppet can be used to automate the build and configuration of dnsmasq DNS with the help of Puppet, and without dnsmasq How Puppet can be used to provide a poor mans DNS system without dnsmasq And then the infrastructure: Puppet Enterprise 2016.2.1 Red Hat Enterprise Linux 6 for the puppet master Red Hat Enterprise Linux 7 for the dnsmasq server DHCP enabled network I am using Puppet Enterprise 2016.2.1 in my home lab, taking advantage of the free 10 node license. In order to setup DNS for your home network, you don’t need Puppet, but it comes in handy when dynamically deploying VMs that get an IP via DHCP and you want it automatically registered with DNS. With that, Puppet is not required to setup dnsmasq, nor is a DHCP enabled network should you assign static IPs to your lab systems. You can still follow along with the dnsmasq portion of this article and still come out with DNS for your home network / lab. – Red Hat Developer NetworkFor those who aren’t aware, you can now get Red Hat Enterprise Linux for free. Simply sign up at https://developers.redhat.com/ and you’ll have access to a no-cost Red Hat Enterprise Linux Developer Suite subscription. This article will reference the use of Red Hat, but of course CentOS or any distro will do as the dnsmasq configs will be the same. dnsmasq (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: \"ca-pub-8101539885827084\", enable_page_level_ads: true }); Let’s talk about dnsmasq first, and we’ll tie in the Puppet piece later. I’ve chosen to run my dnsmasq server in Red Hat 7. Our dnsmasq instance is only going to serve up DNS. We will NOT use the DHCP and TFTP capabilities of dnsmasq for this use case. I refer to my home lab as 3031.net. I do not own this domain, and I’m not really sure who does. I don’t care either, but I want to use it, and I can setup dnsmasq to resolve requests for 3031.net any way I’d like. You can do the same, but obviously, you wouldn’t want to use something that you use often, like google.com. Goes without saying I think… With that, chose your name with care, and start to set things up. (For those intereted in the Puppet part, the things done manually here are puppetized further down in the article. At least continue reading from this point for the sake of context.) Deploy a Linux VM as the dnsmasq server Ensure that the dnsmasq package is installed We will configure dnsmasq to use the local /etc/hosts file to contain and serve up the the DNS records Leave /etc/dnsmasq.conf alone. Instead, place your dnsmasq configurations under /etc/dnsmasq.d. This is not a requirement, but just something I’ve opted to do. For example, I call my home lab 3031.net so I named my config file 3031.net 3031 vmwdnsmasq ~ # cat /etc/dnsmasq.d/3031.net domain-needed bogus-priv domain=3031.net expand-hosts local=/3031.net/ no-dhcp-interface=ens160 no-resolv no-poll server=8.8.8.8 server=8.8.4.4 Refer to what these various options mean by looking at /etc/dnsmasq.conf. In short: domain-needed Block incomplete requests from leaving your network, such as google vs. google.com bogus-priv Bogus private reverse lookups. All reverse lookups for private IP ranges (ie 192.168.x.x, etc) which are not found in /etc/hosts or the DHCP leases file are answered with “no such domain” rather than being forwarded upstream. no-resolv Don’t read /etc/resolv.conf. Get upstream servers only from the command line or the dnsmasq configuration file no-poll Don’t poll /etc/resolv.conf for changes domain and expand-hosts play together. What this allows is for a DNS lookup to succeed if someone conducts a lookup on a fully qualified domain name, but only the short name exists in /etc/hosts. For instance, the ‘what’ entry below is only the short name: 3031 vmwdnsmasq ~ # cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.1.3 vmwdnsmasq.3031.net vmwdnsmasq 192.168.1.254 what 3031 vmwdnsmasq ~ #3031 vmwdnsmasq ~ # nslookup what.3031.net 192.168.1.3 Server: 192.168.1.3 Address: 192.168.1.3#53 Name: what.3031.net Address: 192.168.1.254 If those two options were not set, the lookup would only work against the short name, and fail on the FQDN: 3031 vmwdnsmasq ~ # nslookup what Server: 192.168.1.1 Address: 192.168.1.1#53 Name: what Address: 192.168.1.254 vs. 3031 vmwdnsmasq ~ # nslookup what.3031.net 192.168.1.3 Server: 192.168.1.3 Address: 192.168.1.3#53 ** server can't find what.3031.net: NXDOMAIN local ensures that queries for your private domain are only answered by dnsmasq, from /etc/hosts. In the 3031.net example, I can set local=/3031.net/ to tell dnsmasq never resolve this domain outside of what I’ve setup in /etc/hosts. That way, I will never get the real Internet facing IP address to anything 3031.net. no-dhcp-interface turns off DHCP and TFTP, and provides DNS service only. The server lines inform dnsmasq where for forward Internet DNS requests. The upstream nameservers have been set to Google DNS. That’s it for the dnsmasq setup! Add whatever you want resolved to the local /etc/hosts file on the dnsmasq server, restart the dnsmasq service, and resolve away! At this point, implement dnsmasq into your lab however you see fit. Further reading below outlines my implementation. Putting it to useIs the DNS service to be used by lab systems only? If you’re simply in need of a DNS system for your lab systems only, you could call it done at this point, and manually manage your dnsmasq config from this point going foward. If you’re manually deploying lab VMs and assigning static IPs, you can simply point DNS for those interfaces to your new dnsmasq server, update /etc/hosts on the dnsmasq server, bounce the dnsmasq service, and allow for name resolution within your lab systems only. Manual management of the config may be okay if you are only deploying a handful of machines, and just need to get something setup and functional. Is the DNS service to be used for the entire home network? For me, I wanted to take it a step further. In my lab, I auto deploy VMs to my vCenter instance directly from my laptop using vagrant (see this article yet to be linked). Each VM is assigned an IP via DHCP via my router, and I want this IP to auto-register itself with the dnsmasq server and become resolvable automatically, without having to manually inject the information into the dnsmasq servers /etc/hosts file. I also want to be able to resolve my lab systems from any system on the home network, including my laptop. To accomplish the latter, I pointed my routers DNS servers to my dnsmasq server (which is staically IP’d), instead of my ISPs DNS servers. This means less configuration on the side of dnsmasq to take over DHCP handouts, and setup of dhcp-options to ensure proper router configurations are being handed out. Changing the DNS servers of the router banks heavily on the stability and uptime of the dnsmasq server of course. Obviously, if it went down, I will have no DNS resolution until the server is back up. For me, this is acceptable. My Lenovo TS140 that runs vCenter is on a battery backup with an uptime of 540+ days, where my router gets rebooted periodically and is not on a battery backup. So far, this setup has been extremely reliable. Cool – now that we’ve got DNS resolution for lab systems and Internet requests, where does Puppet come into play? Exported resources!! PuppetHow Puppet can be used to automate the management of DNS entries Stay tuned for more indepth coverage on this via the video session! Using exported resources, we can publish the resources of individual nodes for use by other nodes. Think about it… Most things in puppet are a resource; package, file, service, cron, mount, host. Using Puppet, we can export the host resource of each VM, and collect the exported resources across other nodes, including our newly created dnsmasq server. Note that in order to use exported resources, you must have a PuppetDB deployment. This is where exported resources are stored. To accomplish this, here is a simple host module that exports AND collects the exported host resource. (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: \"ca-pub-8101539885827084\", enable_page_level_ads: true }); class hosts &#123; resources &#123;'host': purge =&gt; true, &#125; host &#123; 'localhost': ensure =&gt; present, host_aliases =&gt; ['localhost.localdomain', 'localhost4', 'localhost4.localdomain4'], ip =&gt; '127.0.0.1', &#125; @@host &#123; $facts['fqdn']: ensure =&gt; present, host_aliases =&gt; $facts['hostname'], ip =&gt; $facts['ipaddress'], tag =&gt; '3031', &#125;&#125; Simply classify all of your lab systems with the host class, and on subsequent puppet runs, it’ll export the systems hostname and ipaddress on all nodes classified with the host class. In addition, any /etc/hosts entries not managed via the hosts class will be purged. A quick note on purging… Puppet will purge anything that it doesn’t know about when setting purge =&gt; true. Be careful when using this if there are local /etc/host entries that need to be kept and haven’t been declared as a host resource within the host class. In addition, if a node is destroyed, the exported resources won’t be removed from PuppetDB until they too, have been purged from PuppetDB, meaning purging the node on the puppet master via the ‘puppet node purge &lt;puppet node cert&gt;‘ command. You’ll notice that the resource collector is not included in the hosts class. If you want to populate each systems local /etc/hosts file, a collector may be added here. Otherwise, I’ve personally opted NOT to do that, and instead, want to collect the exported resource on the dnsmasq server only, as shown below. As part of the resource collector, the dnsmasq service will be notified each time an exported resource tagged as 3031 is collected. That way, newly added DNS records input into /etc/hosts will be available for name resolution. Puppetizing dnsmasqHow Puppet can be used to automate the build and configuration of dnsmasq ktreese puppet-control (classify with the role::dnsmasq class contained here) ktreese/dnsmasq class dnsmasq ( $network_ip = $::dnsmasq::params::network_ip, $network_netmask = $::dnsmasq::params::network_netmask, $network_gateway = $::dnsmasq::params::network_gateway, $network_bootproto = $::dnsmasq::params::network_bootproto, $network_onboot = $::dnsmasq::params::network_onboot, $network_dns1 = $::dnsmasq::params::network_dns1, $network_defroute = $::dnsmasq::params::network_defroute, $network_iface = $::dnsmasq::params::network_iface, $network_domain = $::dnsmasq::params::network_domain,) inherits ::dnsmasq::params &#123; package &#123; 'dnsmasq': ensure =&gt; present, &#125; file &#123; \"/etc/dnsmasq.d/$&#123;network_domain&#125;\": ensure =&gt; present, content =&gt; template(\"dnsmasq/$&#123;network_domain&#125;.erb\"), owner =&gt; 'root', group =&gt; 'root', mode =&gt; '0644', notify =&gt; Service['dnsmasq'], require =&gt; Package['dnsmasq'], &#125; service &#123; 'dnsmasq': ensure =&gt; running, enable =&gt; true, require =&gt; Package['dnsmasq'], &#125; network::interface &#123; $network_iface: ipaddress =&gt; $network_ip, netmask =&gt; $network_netmask, gateway =&gt; $network_gateway, bootproto =&gt; $network_bootproto, onboot =&gt; $network_onboot, dns1 =&gt; $network_dns1, defroute =&gt; $network_defroute, &#125; Host &lt;&lt;| tag == '3031' |&gt;&gt; ~&gt; Service['dnsmasq']&#125; ➜ dnsmasq git:(master) cat templates/dnsmasq/3031.net domain-needed bogus-priv domain=&lt;%= @network_domain %&gt; expand-hosts local=/&lt;%= @network_domain %&gt;/ no-dhcp-interface=&lt;%= @network_iface %&gt; no-resolv no-poll server=8.8.8.8 server=8.8.4.4 DNS with the help of Puppet, and without dnsmasq Another approach would be to bypass the use of dnsmasq altogether, and instead, add a resource collector to the host module, and use it to populate the /etc/hosts file on every system, thus relying on local /etc/hosts entries for name resolution. You may consider installing the puppet agent on your laptop as well, and classify it with the host class so that it too, will get the exported host resource of your lab gear, therefore allowing your laptop to resolve against /etc/hosts. Lots of ways to approach this… Let me know what you think! Limitations Exported resources should be immediately available upon new system deployments and their first puppet run, assuming newly deployed systems match the Node Group configured with the host class. However, DNS Record updates will become available as frequent as the runinterval of the puppet agent on the dnsmasq server. Provided this is a home lab, either perform a puppet run on the dnsmasq server via command line, from the Puppet Console, via mco, or set the runinterval to run every n seconds, whatever works for your setup. For me, I’m not deploying VMs frequently enough to warrant the dnsmasq server checking in with the puppet master every 30 seconds, for example, so I left mine set to default, and opt’d to force a puppet run in one of the ways noted above.","categories":[{"name":"Puppet","slug":"Puppet","permalink":"http://kristianreese.com/categories/Puppet/"}],"tags":[{"name":"Puppet","slug":"Puppet","permalink":"http://kristianreese.com/tags/Puppet/"},{"name":"dnsmasq","slug":"dnsmasq","permalink":"http://kristianreese.com/tags/dnsmasq/"}],"author":"Kris Reese"},{"title":"Convert Single Cat 5e into Ethernet and Phone","slug":"Convert-Single-Cat-5e-into-Ethernet-and-Phone","date":"2019-05-06T11:42:00.000Z","updated":"2019-05-06T02:39:25.000Z","comments":true,"path":"2019/05/06/Convert-Single-Cat-5e-into-Ethernet-and-Phone/","link":"","permalink":"http://kristianreese.com/2019/05/06/Convert-Single-Cat-5e-into-Ethernet-and-Phone/","excerpt":"","text":"How to convert a single Cat5e connection into an Ethernet &amp; Phone lineMy new home was prewired with Cat5e cable for telephone. While this is nice, I didn’t want to be tied down to phone-only mainly because I prefer to have the option of placing my wireless router on the 1st story in order to have a stronger signal on both the 1st and 2nd story, and when I’m outside in the backyard so I can bump Spotify while mowing the lawn and not chew up my data plan. Here’s how I converted the phone line that ran into my office into both Ethernet and phone:Shopping List: I purchased the following from amazon.com: Cooper Wiring Devices 5547-3EW Cat 3 RJ11 Modular Voice Jack Insert, White Monoprice Cat5e Punch Down Keystone Jack Icarus Keystone Style Wall Plate - 2 Ports TRENDnet 8P/RJ-45 and 6P/RJ-12, RJ-11 Crimp, Cut, and Strip Tool, TC-CT68 R.J. Enterprises 50-Pack RJ-45 Connectors (Clear) To begin, remove the RJ11 wall plate. Once removed from the wall, you’ll see which twisted pair of copper wire are connected to the jack of the wall plate. There are two standards of wiring called T568A and T568B, where the recommended standard for residental dwellings based on the “Residential Telecommunications Cabling Standards” is T568A, which I assumed the builder has followed. Here are two diagrams depicting these two standards: Figure 1: View of wired Ethernet connector, showing cable layout for each standard Figure 2: another diagram from wikipedia showing Pin to Pair mapping per standard In my case, Pair 1 &amp; 2 were being used for telephone, which left Pair 3 &amp; 4 unused. Here is a picture of what mine looked like: You can see the white/orange, orange, white/blue, blue cables connected to the screws on the plate. These same cables (Pair 1 and Pair 2 per the T568A standard) will be used in the new Cat3 RJ11 adapter (first bullet point item of shopping list) by simply removing them from the wall plate, and inserting them into the respective punch down slots of the adapter . Be sure to insert them into the same color code on the RJ11 adapter as it was on the wall plate. (You may have noticed the wall plate also has color coded wiring). Before I removed the cables from the wall plate, I wrote down the color mapping just in case (see image below), where the / represents the color white (ie “orange/“ means White/Orange). Here’s a diagram to illustrate what it should look like, and we can see it matches up with what I’ve written down: Pins 2,3,4,5 are the pin outs for phone on the RJ11 adapter. These pin numbers are clearly labeled on the adapter just as they are color coded (per the handwritten mapping). Note: here is a picture of my setup below. In this picture below, I installed the wires into the adapter, tested to make sure it worked, then snapped the adapter into the Keystone 2 port wall plate. (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: \"ca-pub-8101539885827084\", enable_page_level_ads: true }); Now for the InternetIn my experience of making cables, I’m used to the T568B standard. You can confirm this yourself by looking at the tip of a network cable (see Figure 1 above) and see that the color of the wires within are in the Pin positions shown in the T568B diagram above. With that said, the Pin Position is what will be important when making the connection for Ethernet since electronically, cable color doesn’t matter. Just don’t confuse yourself like I did since Pair 1 and Pair 2 per T568A (the white/blue blue and white/orange orange) cables are already used. In this case, I referenced T568B and I opted to turn the White/Brown Brown pair into pin positions 1 and 2 to serve as Pair 2 (the White/Orange Orange pair) in the T568B standard, and White/Green Green pair into pin positions 3 and 6 to serve as themselves in Pair 3.I probably should have kept in the T568A standard and left Pair 3 (White/Green Green) to the pin 1 and pin 2 position. Again doesn’t really matter as long as the correct pin positions for an Ethernet connection have a wire and match on the other end. Go into the basement and find the Cat5e cable that runs into the room where you want to convert into phone &amp; Ethernet. Using a RJ45 connector and the crimpers, carefully insert the twister pairs into their respective Pin out locations. Be sure they are pressed firmly to the tip of the connector and remain there as the connector is crimped. This is well depicted in the image below where you will see Pair 4 (white/brown brown) in the Pin 1 and Pin 2 position, and Pair 3 in Pin 3 and Pin 6 position. (adsbygoogle = window.adsbygoogle || []).push({ google_ad_client: \"ca-pub-8101539885827084\", enable_page_level_ads: true }); Plug this end into your DSL or cable modem: Go back into the room where you want to make two connections. Grab the RJ45 adapter you purchased from the shopping list and following the numbers on the adapter, put the cables into the numbered Pin positions, just as you did with the basement connection: Test the connection to make sure it works. If it does, then you’re done! Cut the ends on the outter side of the adapter and button everything up and you’re wall plate should look like this! Notes:I think the phone only requires one pair (at least it did in my last house), so I could have tested by using the T568A standard to wire things up as follows: Phone Pair 1 -&gt; White/Blue Blue -&gt; RJ11 adapter punch down slots 4 &amp; 3 respectively Ethernet Pair 3 &amp; Pair 2 -&gt; White/Green Green, White/Orange Orange -&gt; RJ45 adapter punch down slots 1,2, 6,3 respectively If I change another outlet in the house, I will likely try it this way, and take more pictures to better illustrate the process. This would leave pair 4 unused. One other note is that as long as your home won’t be using 1 GBps connections, then it’s possible to split the twisted pair of one cable for phone and Ethernet. Otherwise, if your home will require 1 GBps (1000MBps) connections, then this cannot be accomplished since 1 GBps connections require all 4 sets of the twisted pair. END","categories":[{"name":"Networking","slug":"Networking","permalink":"http://kristianreese.com/categories/Networking/"}],"tags":[{"name":"Home Projects","slug":"Home-Projects","permalink":"http://kristianreese.com/tags/Home-Projects/"},{"name":"Networking","slug":"Networking","permalink":"http://kristianreese.com/tags/Networking/"}],"author":"Kris Reese"},{"title":"Zero byte a File","slug":"zero-byte-a-file","date":"2019-05-05T16:50:00.000Z","updated":"2019-05-07T14:56:46.000Z","comments":true,"path":"2019/05/05/zero-byte-a-file/","link":"","permalink":"http://kristianreese.com/2019/05/05/zero-byte-a-file/","excerpt":"","text":"/dev/null (zero out) a bunch of files at oncecd /var/logcat /dev/null | tee &#123;boot.log,cron,dmesg,lastlog,messages,secure,vzctl.log,vznetcfg.log,wtmp&#125; &gt;boot.log&gt;cron&gt;dmesg&gt;messages&gt;secure&gt;vzctl.log&gt;vznetcfg.log&gt;wtmp","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"netmasks table","slug":"netmasks-table","date":"2019-05-05T11:50:00.000Z","updated":"2019-05-07T01:56:03.000Z","comments":true,"path":"2019/05/05/netmasks-table/","link":"","permalink":"http://kristianreese.com/2019/05/05/netmasks-table/","excerpt":"","text":"Decimal Binary CIDR Hex-------------------------------------------------------------------255.255.255.255 11111111.11111111.11111111.11111111 /32 ffffffff255.255.255.254 11111111.11111111.11111111.11111110 /31 fffffffe255.255.255.252 11111111.11111111.11111111.11111100 /30 fffffffc255.255.255.248 11111111.11111111.11111111.11111000 /29 fffffff8255.255.255.240 11111111.11111111.11111111.11110000 /28 fffffff0255.255.255.224 11111111.11111111.11111111.11100000 /27 ffffffe0255.255.255.192 11111111.11111111.11111111.11000000 /26 ffffffc0255.255.255.128 11111111.11111111.11111111.10000000 /25 ffffff80255.255.255.0 11111111.11111111.11111111.00000000 /24 ffffff00255.255.254.0 11111111.11111111.11111110.00000000 /23 fffffe00255.255.252.0 11111111.11111111.11111100.00000000 /22 fffffc00255.255.248.0 11111111.11111111.11111000.00000000 /21 fffff800255.255.240.0 11111111.11111111.11110000.00000000 /20 fffff000255.255.224.0 11111111.11111111.11100000.00000000 /19 ffffe000255.255.192.0 11111111.11111111.11000000.00000000 /18 ffffc000255.255.128.0 11111111.11111111.10000000.00000000 /17 ffff8000255.255.0.0 11111111.11111111.00000000.00000000 /16 ffff0000255.254.0.0 11111111.11111110.00000000.00000000 /15 fffe0000255.252.0.0 11111111.11111100.00000000.00000000 /14 fffc0000255.248.0.0 11111111.11111000.00000000.00000000 /13 fff80000255.240.0.0 11111111.11110000.00000000.00000000 /12 fff00000255.224.0.0 11111111.11100000.00000000.00000000 /11 ffe00000255.192.0.0 11111111.11000000.00000000.00000000 /10 ffc00000255.128.0.0 11111111.10000000.00000000.00000000 /9 ff800000255.0.0.0 11111111.00000000.00000000.00000000 /8 ff000000254.0.0.0 11111110.00000000.00000000.00000000 /7 fe000000252.0.0.0 11111100.00000000.00000000.00000000 /6 fc000000248.0.0.0 11111000.00000000.00000000.00000000 /5 f8000000240.0.0.0 11110000.00000000.00000000.00000000 /4 f0000000224.0.0.0 11100000.00000000.00000000.00000000 /3 e0000000192.0.0.0 11000000.00000000.00000000.00000000 /2 c0000000128.0.0.0 10000000.00000000.00000000.00000000 /1 800000000.0.0.0 00000000.00000000.00000000.00000000 /0 00000000","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"},{"name":"Networking","slug":"Linux/Networking","permalink":"http://kristianreese.com/categories/Linux/Networking/"}],"tags":[{"name":"Networking","slug":"Networking","permalink":"http://kristianreese.com/tags/Networking/"},{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"nmap commands","slug":"nmap","date":"2019-05-05T11:50:00.000Z","updated":"2019-05-07T14:56:55.000Z","comments":true,"path":"2019/05/05/nmap/","link":"","permalink":"http://kristianreese.com/2019/05/05/nmap/","excerpt":"","text":"Scan network for responsive IPsnmap -sS -O 10.199.42.0/23 | tee nmap.log Check if port is opennmap -P0 -p1521 oracle.com Scan UDPnmap -sU -p &lt;port&gt; &lt;target&gt;","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"commands","slug":"commands","permalink":"http://kristianreese.com/tags/commands/"}],"author":"Kris Reese"},{"title":"sed Commands","slug":"sed-commands","date":"2019-05-05T11:50:00.000Z","updated":"2019-05-07T14:57:12.000Z","comments":true,"path":"2019/05/05/sed-commands/","link":"","permalink":"http://kristianreese.com/2019/05/05/sed-commands/","excerpt":"","text":"sed one linersInsert character at beginning of lineComment all nfs entries in fstabsed -i &#39;/nfs/s/^/#/&#39; /etc/fstab Uncomment all nfs entries in fstabsed -i &#39;/nfs/s/^#//g&#39; /etc/fstab Substitute a character at a specific lineChange line 108 of /etc/sudoers by removing the comment and immediate space that follows from wheel group sed -i -e &#39;108s/# //g&#39; /etc/sudoers Insert a string at a specific line numbersed -i &#39;14iparser = future&#39; /etc/puppetlabs/puppet/puppet.conf copy a file / sub string in originalperl -i.orig2 -p -e &#39;s/10.49.5.61/10.49.5.62/g&#39; /etc/fstab Print contents of a file between lines 1 and 609sed -n &#39;1,609&#39;p /var/log/tspulsemail.log &gt; index.html Substitute line feed with a spaceawkawk &#39;{ print $1 }&#39; | tr &#39;\\n&#39; &#39; &#39; &lt; file perlperl -p -e &#39;s/\\n/ /&#39; file Interpolating env vars vs. EOLTo use BOTH Unix $environment_vars and sed /end-of-line$/ pattern matching Excerpt taken from: sed.sourceforge.net The most readable is to enclose the script in “double quotes” so the shell can see the $variables, and to prefix the sed metacharacter ($) with a backslash: sed &quot;s/$user\\$/root/&quot; file the shell interpolates $user and sed interprets \\$ as the symbol for end-of-line. Another method is to concatenate the script with ‘single quotes’ where the $ should not be interpolated and “double quotes” where variable interpolation should occur: sed &quot;s/$user&quot;&#39;$/root/&#39; file","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"commands","slug":"commands","permalink":"http://kristianreese.com/tags/commands/"}],"author":"Kris Reese"},{"title":"tar / gzip commands","slug":"tar-gzip","date":"2019-05-05T11:50:00.000Z","updated":"2019-05-07T14:58:32.000Z","comments":true,"path":"2019/05/05/tar-gzip/","link":"","permalink":"http://kristianreese.com/2019/05/05/tar-gzip/","excerpt":"","text":"Here, we are sending the tar file to -, which stands for tar’s standard output. This is piped to gzip, which compresses the incoming tar file, and the result is saved in backup.tar.gz. The -c option to gzip tells gzip to send its output to stdout, which is redirected to backup.tar.gz. tar cvf - /etc | gzip -9c &gt; backup.tar.gz A single command used to unpack this archive would be: gunzip -c backup.tar.gz tar xvf - -or- gunzip -c LDAPsetup.1.0.SPARC.SOL9.pkg.tar.gz | tar -xvf - Create a tar file from list of files inside a text filetar cfvz forhans.tar.gz `cat manifest.txt` warning if there are spaces in the file, you will have issues Backup home directory(cd /home/reese; tar cf - *) &gt; reese-hostname.tar","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"commands","slug":"commands","permalink":"http://kristianreese.com/tags/commands/"}],"author":"Kris Reese"},{"title":"tcpdump commands","slug":"tcpdump-commands","date":"2019-05-05T11:50:00.000Z","updated":"2019-05-07T01:47:19.000Z","comments":true,"path":"2019/05/05/tcpdump-commands/","link":"","permalink":"http://kristianreese.com/2019/05/05/tcpdump-commands/","excerpt":"","text":"Capture DHCP requestsPXE pxedhcp01 ~ # tcpdump -i eth1 \\(port 67 or port 68\\)tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth1, link-type EN10MB (Ethernet), capture size 96 bytes16:12:28.564974 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 00:21:5e:09:60:80 (oui Unknown), length 54816:12:28.565003 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 00:21:5e:09:60:80 (oui Unknown), length 54816:12:28.565449 IP 10.30.71.10.67 &gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 31616:12:28.565545 IP 10.30.71.10.67 &gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 31616:12:32.602043 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 00:21:5e:09:60:80 (oui Unknown), length 54816:12:32.602119 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 00:21:5e:09:60:80 (oui Unknown), length 54816:12:32.602353 IP 10.30.71.10.67 &gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 31616:12:32.602537 IP 10.30.71.10.67 &gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 31616:12:36.666578 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 00:21:5e:09:60:80 (oui Unknown), length 54816:12:36.666629 IP 0.0.0.0.68 &gt; 255.255.255.255.67: BOOTP/DHCP, Request from 00:21:5e:09:60:80 (oui Unknown), length 54816:12:36.666911 IP 10.30.71.10.67 &gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 31616:12:36.667052 IP 10.30.71.10.67 &gt; 255.255.255.255.68: BOOTP/DHCP, Reply, length 316^C12 packets captured12 packets received by filter0 packets dropped by kernelPXE pxedhcp01 ~ # tcpdump -n -i any port 67 or port 68 or port 69 will also work Capture specific MAC addresstcpdump &quot;ether host 00:14:5e:3c:93:ea&quot; Capture all traffic except sshtcpdump -n -i eth0 not port ssh","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"commands","slug":"commands","permalink":"http://kristianreese.com/tags/commands/"}],"author":"Kris Reese"},{"title":"Top CPU & Memory Processes","slug":"top-cpu-mem-processes","date":"2019-05-05T11:50:00.000Z","updated":"2019-05-07T14:56:35.000Z","comments":true,"path":"2019/05/05/top-cpu-mem-processes/","link":"","permalink":"http://kristianreese.com/2019/05/05/top-cpu-mem-processes/","excerpt":"","text":"Display top 10 processes eating up CPUDisplay by processps -eo pcpu,pid,user,args | sort -k 1 -r | head -10 Display by userps -U apache -u apache u Display top 10 processes eating up memoryps axo %mem,pid,euser,cmd | sort -nr | head -n 10ps -e -orss=,args= | sort -b -k1,1n | pr -TW$COLUMNS","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"How to Add Swap Space","slug":"add-swap-space","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T14:57:03.000Z","comments":true,"path":"2019/05/05/add-swap-space/","link":"","permalink":"http://kristianreese.com/2019/05/05/add-swap-space/","excerpt":"","text":"How To Add Swap Space to a Linux SystemThere are two ways to go about adding swap space to a linux system. Use of a hard disk partition Create a swap file on an existing file system To view current swap utilization, a user can use the commands free or swapon. Swap utilization may also be viewed within the /proc file system: [root@linux01 ~]# free -m total used free shared buffers cachedMem: 48297 34139 14157 0 556 25966-/+ buffers/cache: 7616 40681Swap: 10236 49 10187[root@linux01 ~]# swapon -sFilename Type Size Used Priority/dev/sda3 partition 10482404 50364 -1[root@linux01 ~]# cat /proc/swaps Filename Type Size Used Priority/dev/sda3 partition 10482404 50364 -1 Clearly, we see swap -s and /proc/swaps provides the same output Method 1Disk partitionUse of a hard disk partition is very simple to setup. If a spare hard disk is available, or there’s unpartitioned space available on an existing hard disk, use fdisk to create a partition. In my case, I had neither immediately available to me, but what I did have was plenty of EMC SAN, so I created a LUN, presented it to the server, and created a partition via fdisk just as if it were a regular disk. While creating the partition, don’t forget to set the partition type to Linux swap ==Hex code 82== via the ==t== option in fdisk: t change a partition&#39;s system id In the case of VMware, a virtual disk can be added. In this case, or in the case of adding a new physical disk to a server, here’s how to add the disk to the system without rebooting, minus the need to create the ext3 filesystem of course. Here is what mine looked like after creating the partition via fdisk: [root@linux01 ~]# fdisk -l /dev/emcpowerajDisk /dev/emcpoweraj: 32.2 GB, 32212254720 bytes64 heads, 32 sectors/track, 30720 cylindersUnits = cylinders of 2048 * 512 = 1048576 bytes Device Boot Start End Blocks Id System/dev/emcpoweraj1 1 30720 31457216 82 Linux swap Next, use the mkswap command to set up a Linux swap area, and swapon to enable the device for swapping [root@linux01 ~]# mkswap /dev/emcpoweraj1[root@linux01 ~]# swapon /dev/emcpoweraj1 In order to make this swap partition available across reboots, don’t forget to add it to the /etc/fstab file. !!! note “fstab entry” ## xtra swap via EMC LUN/dev/emcpoweraj1 swap swap defaults 0 0 Verify the new swap space is available to the system: [root@linux01 ~]# cat /proc/swaps Filename Type Size Used Priority/dev/sda3 partition 10482404 50360 -1/dev/emcpoweraj1 partition 31457208 0 -2[root@linux01 ~]# free -m total used free shared buffers cachedMem: 48297 32580 15717 0 562 26390-/+ buffers/cache: 5626 42671Swap: 40956 49 40907 Method 2Free disk spaceIf you don’t have additional, physical hardware or non-partitioned space available, but have plenty of free disk space within an existing filesystem, you can create a file and use that for swap space. Here’s an example of adding 2 GB of swap via a swap file: [root@linux01 ~]# dd if=/dev/zero of=/root/xtraswap bs=1M count=20482048+0 records in2048+0 records out[root@linux01 ~]# ls -lh /root/xtraswap -rw-r--r-- 1 root root 2.0G Jul 23 14:08 /root/xtraswap Permissions should be changed such that only root has access, then setup the linux swap area via mkswap and enable it [root@linux01 ~]# chmod 600 /root/xtraswap[root@linux01 ~]# mkswap /root/xtraswapSetting up swapspace version 1, size = 2147479 kB[root@linux01 ~]# swapon /root/xtraswap Just as in method 1 above, verify the extra swap space has been added by looking at /proc/swaps or using the swap -s command. In addition, add the newly created swap space to /etc/fstab to ensure it’s available on reboot. !!! note “fstab entry” ## xtra swap via swap file/root/xtraswap swap swap defaults 0 0","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"/boot/grub/stage1 not read correctly","slug":"boot-grub-stage1","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T02:59:25.000Z","comments":true,"path":"2019/05/05/boot-grub-stage1/","link":"","permalink":"http://kristianreese.com/2019/05/05/boot-grub-stage1/","excerpt":"","text":"After pxe booting a server and installing linux from a cpio archive, grub failed to load using the grub-install command. The error presented was as follows: root@pxe:~# mkdir /targetroot@pxe:~# mount /dev/sda1 /targetroot@pxe:~# chroot /targetroot@pxe:~# grub-install /dev/sdaThe file /boot/grub/stage1 not read correctly. Since command line didn’t work, I tried installing grub via the grub shell: root@pxe:~# grub GNU GRUB version 0.97 (640K lower / 3072K upper memory) [ Minimal BASH-like line editing is supported. For the first word, TAB lists possible command completions. Anywhere else TAB lists the possible completions of a device/filename.]grub&gt; root (hd0,0)root (hd0,0) Filesystem type is ext2fs, partition type 0xfdgrub&gt; setup (hd0)setup (hd0) Checking if \"/boot/grub/stage1\" exists... no Checking if \"/grub/stage1\" exists... no Error 2: Bad file or directory typegrub&gt; This too failed, with a different error message. You may have noticed the first error message given while attempting to install grub with the grub-install command yielded: The file /boot/grub/stage1 not read correctly While attempting to install grub via the grub shell yielded: Error 2: Bad file or directory type After some research, grub only works on an inode size of 128, and the pxe system formatted the file system with an inode size of 256. root@pxe:~# tune2fs -l /dev/sda1 | grep -i 'inode size'Inode size: 256 The system being reimaged was an older legacy system, and the pxe system had been upgraded months prior. Having remembered this, I compared versions of mke2fs and found that the upgraded version creates 256-byte inodes by default, where the older version was 128. To correct this, I updated the pxe imaging scripts responsible for formatting the disks with the -I option as follows: root@pxe:~# mke2fs -L / -I 128 -F -j -O dir_index /dev/sda1 Once recreating the file system with a defined inode size of 128, the grub-install command worked successfully. root@pxe:~# tune2fs -l /dev/sda1 | grep -i 'inode size'Inode size: 128","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Enable auto logoff","slug":"auto-logoff","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T14:57:44.000Z","comments":true,"path":"2019/05/05/auto-logoff/","link":"","permalink":"http://kristianreese.com/2019/05/05/auto-logoff/","excerpt":"","text":"Enabling automatic logoffSystems can be set to automatically logoff a user after a period of activity by setting some parameters: Linux/Solaris /etc/profile AIX /etc/security/.profile Parameter declarations are the same across platforms. Append the following to the system profile: #Enable automatic logoffTMOUT=600TIMEOUT=600readonly TMOUT TIMEOUTexport TMOUT TIMEOUT tip Set readonly to prevent the variables from being overwritten.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Create temp file with random characters","slug":"create-a-temp-file-with-random-characters","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T14:55:33.000Z","comments":true,"path":"2019/05/05/create-a-temp-file-with-random-characters/","link":"","permalink":"http://kristianreese.com/2019/05/05/create-a-temp-file-with-random-characters/","excerpt":"","text":"tmpfile=`/bin/mktemp /var/log/rpmpkgs.XXXXXXXXX` || exit 1","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Set static and default gateway in route-eth file","slug":"defining-static-and-default-gateway-in-route-eth-file","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:00:33.000Z","comments":true,"path":"2019/05/05/defining-static-and-default-gateway-in-route-eth-file/","link":"","permalink":"http://kristianreese.com/2019/05/05/defining-static-and-default-gateway-in-route-eth-file/","excerpt":"","text":"Define default gateway in route-eth file[root@linux01 ~]# cat /etc/sysconfig/network-scripts/route-eth1default via 10.195.30.1 dev eth1 Define default gateway in /etc/sysconfig/network:[root@linux01 ~]# cat /etc/sysconfig/networkNETWORKING=yesHOSTNAME=linux01GATEWAY=10.195.30.1 Define static and default route in respective route-eth file[root@linux02 ~]# cat /etc/sysconfig/network-scripts/route-eth010.0.0.0/8 via 10.30.118.110.30.33.14 via 10.30.118.1 dev eth0 [root@linux02 ~]# cat /etc/sysconfig/network-scripts/route-eth1default via 10.30.119.1 dev eth1","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"cron tips","slug":"cron","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:00:05.000Z","comments":true,"path":"2019/05/05/cron/","link":"","permalink":"http://kristianreese.com/2019/05/05/cron/","excerpt":"","text":"Cron tipscrontab formatHere’s a quick comment that can be added to users crontabs as a reminder to the crontab format: # m h dom mon dow command There are various ways to schedule a command to run, such as every 5 minutes, that would be considered acceptable format by some linux distributions, and not accepted by Solaris. This kb article was intended to be kept simple as a reminder to what order the fields go, something that can be found easily by reading the man page. [root@linux01 ~]# crontab -l# m h dom mon dow command## ssh blackhole to prevent brute force attacks* * * * * /root/scripts/ssh-blackhole.bsh &gt;/dev/null 2&gt;&amp;1#* * * * * /root/scripts/ssh-blackhole2.bsh &amp;&gt;/tmp/ssh-blackhole.troubleshoot.log # For details see man 4 crontabs# Example of job definition:# .---------------- minute (0 - 59)# | .------------- hour (0 - 23)# | | .---------- day of month (1 - 31)# | | | .------- month (1 - 12) OR jan,feb,mar,apr ...# | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat# | | | | |# * * * * * user-name command to be executed (full path name to command must be used) Disable cron based mail alertsThe easiest way to disable cron based email alerts is to append the following to the specified command within the users crontab: &gt;/dev/null 2&gt;&amp;1 Example: # m h dom mon dow command59 23 * * * /root/scripts/hotfixes/vzagentdb.sh &gt;/dev/null 2&gt;&amp;1 crond sysconfigCRONDARGS=&#39;-m off&#39; in /etc/sysconfig/crond may also be specified to turn off mail completely. Other postings have also indicated setting the MAILTO=&quot;&quot; option in the users crontab also works. Note that I’ve seen cases where the qmail queue was so high that even after implementing these measures, it seemed to not work, but that was because the qmail queue was draining. Once it completely drained, the madness stopped. I once found a customer was running a cronjob to check the status of httpd every waking moment of uptime. The crontab did not output to /dev/null, and therefore emails were being sent each and everytime the crontjob ran. This script was named “keep-apache-alive” and was subjected as such in the emails, so I looked to see how many of these there were in the users /var/qmail/mailnames/domain.com/info/Maildir/new. There were a LOT, so I blew them away and fixed cron to output to /dev/null, preventing the cronjob from sending emails. -bash-3.1# find . -type f -exec cat &#123;&#125; \\; | grep \"keep-apache-alive\" | wc -l-bash-3.1# find . -type f -exec grep -q \"keep-apache-alive\" &#123;&#125; \\; -exec rm &#123;&#125; \\; -exec echo removed &#123;&#125; \\;-bash-3.1# find . -type f -exec cat &#123;&#125; \\; | grep Subject | more","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Create chroot jail with openssh","slug":"create-chroot-jail-with-openssh","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T14:55:51.000Z","comments":true,"path":"2019/05/05/create-chroot-jail-with-openssh/","link":"","permalink":"http://kristianreese.com/2019/05/05/create-chroot-jail-with-openssh/","excerpt":"","text":"How to create a chroot jail with openssh (sftp only)This write up describes how to utilize openssh to setup a chroot jail for sftp connections within the CentOS family. I tried &amp; tried to install and configure rssh to no avail. I was continually met with “Connection closed” messages which has been documented in the rssh FAQ, and the provided solution did not work. I therefore sought out an alternative solution and was pleased to find a simpler solution using openssh. Check current version of opensshIf you’re not running openssh version 4.9p1 or higher, you must upgrade openssh in order to take advantage of the chroot feature and avoid having to setup an elaborate chroot with libraries and install third-party shells. Start by installing some development toolsyum install -y gcc openssl-devel pam-devel rpm-build Download openssh 5.2p1wget ftp://mirror.planetunix.net/pub/OpenBSD/OpenSSH/portable/openssh-5.2p1.tar.gz Build RPM based off the sourcetar xvfz openssh-5.2p1.tar.gzcp ./openssh-5.2p1/contrib/redhat/openssh.spec /usr/src/redhat/SPECS/cp ./openssh-5.2p1.tar.gz /usr/src/redhat/SOURCES/cd /usr/src/redhat/SPECS/perl -i.bak -pe 's/^(%define no_(gnome|x11)_askpass)\\s+0$/$1 1/' openssh.specrpmbuild -bb openssh.speccd /usr/src/redhat/RPMS/`uname -i` # ls -l-rw-r--r-- 1 root root 275215 Oct 25 16:31 openssh-5.2p1-1.x86_64.rpm-rw-r--r-- 1 root root 437468 Oct 25 16:31 openssh-clients-5.2p1-1.x86_64.rpm-rw-r--r-- 1 root root 275724 Oct 25 16:31 openssh-server-5.2p1-1.x86_64.rpm # rpm -Uvh openssh*rpmPreparing... ########################################### [100%] 1:openssh ########################################### [ 33%] 2:openssh-clients ########################################### [ 67%] 3:openssh-server ########################################### [100%]warning: /etc/pam.d/sshd created as /etc/pam.d/sshd.rpmnewwarning: /etc/ssh/sshd_config created as /etc/ssh/sshd_config.rpmnew mv /etc/pam.d/sshd /etc/pam.d/sshd.origcp /etc/pam.d/sshd.rpmnew /etc/pam.d/sshdmv /etc/ssh/sshd_config /etc/ssh/sshd_config.origcp /etc/ssh/sshd_config.rpmnew /etc/ssh/sshd_config Restart sshd and verify versionservice sshd restart # ssh -VOpenSSH_5.2p1, OpenSSL 0.9.8e-fips-rhel5 01 Jul 2008 !!! info “” After restarting, it may say initlog is obsolete, but, you can ignore as that option is deprecated. Configure sshd_config and restart sshd!!! note “Configure sshd_config and restart sshd” vi /etc/ssh/sshd_config Set the following options at the very end of the file. Note you may have to comment out the sftp-server Subsystem. Also ensure the `Match` directive is at the end of the config file. # override default of no subsystems#Subsystem sftp /usr/libexec/openssh/sftp-serverSubsystem sftp internal-sftpMatch Group sftp ChrootDirectory %h ForceCommand internal-sftp AllowTcpForwarding no !!! important Don’t forget to restart sshd after saving the sshd_config file. Add chroot group/user and set permissionsgroupadd sftpuseradd -d /chroot -u 555 -G sftp -m -s /bin/false sftpuserchown root:root /chrootchmod 0755 /chrootmkdir /chroot/sftpuserchown sftpuser:sftp /chroot/sftpuser Seting the users shell to /bin/false ensures they will never, ever get shell access. You may set the permisisons for your given scenario. The above example would be the proper setup if more than one user is given access, which would not allow any files or directories to be created in the root of the jail (/chroot). Only uploading of files/directories will be allowed within the users “personal” directory (sftpuser). That’s it. sftp to the host and the user will not be able to traverse file systems.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Create logical volume using lvm","slug":"create-logical-volume-using-lvm","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T01:47:11.000Z","comments":true,"path":"2019/05/05/create-logical-volume-using-lvm/","link":"","permalink":"http://kristianreese.com/2019/05/05/create-logical-volume-using-lvm/","excerpt":"","text":"This example uses a local hard disk to create a logical volume. partition the disk as normal using fdisk. I new the fdisk options ahead of time from having done this before. EDS etldev1 ~ # FDISK_CMDLIST=\"n\\np\\n1\\n\\nt\\n8e\\nw\\n\"EDS etldev1 ~ # echo -e -n \"$&#123;FDISK_CMDLIST&#125;\" | ( fdisk /dev/sdc ) This is the same thing as doing it from the fdisk menu as follows: EDS etldev1 ~ # fdisk /dev/sdcWARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u').Command (m for help): nCommand action e extended p primary partition (1-4)pPartition number (1-4): 1First cylinder (1-17769, default 1): Using default value 1Last cylinder, +cylinders or +size&#123;K,M,G&#125; (1-17769, default 17769): Using default value 17769Command (m for help): tSelected partition 1Hex code (type L to list codes): 8eChanged system type of partition 1 to 8e (Linux LVM)Command (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks. Initialize the partition for use by LVM using pvcreate. Create a volume group using the block device specified in pvcreate Create a logical volume in the volume group from step 3 Steps 2, 3, 4 are illustrated below EDS etldev1 ~ # pvcreate /dev/sdc1 Physical volume \"/dev/sdc1\" successfully createdEDS etldev1 ~ # vgcreate vg_dwstore /dev/sdc1 Volume group \"vg_dwstore\" successfully createdEDS etldev1 ~ # vgs VG #PV #LV #SN Attr VSize VFree vg_dwstore 1 0 0 wz--n- 136.11g 136.11g vg_etldev1 1 8 0 wz--n- 135.62g 124.00m vg_etldev1_data 1 1 0 wz--n- 136.12g 0 EDS etldev1 ~ # lvcreate -L 136.11g -n lv_dwstore vg_dwstore Rounding up size to full physical extent 136.11 GiB Logical volume \"lv_dwstore\" createdEDS etldev1 ~ # mkfs.ext4 /dev/vg_dwstore/lv_dwstore mke2fs 1.41.12 (17-May-2010)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks8921088 inodes, 35681280 blocks1784064 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=42949672961089 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872Writing inode tables: done Creating journal (32768 blocks): doneWriting superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 39 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override.EDS etldev1 ~ # EDS etldev1 ~ # mkdir /dwstoreEDS etldev1 ~ # mount /dev/vg_dwstore/lv_dwstore /dwstoreEDS etldev1 ~ # df -Ph /dwstoreFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_dwstore-lv_dwstore 134G 188M 127G 1% /dwstoreEDS etldev1 ~ #","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Failed to activate lv","slug":"failed-to-activate-lv","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T01:46:08.000Z","comments":true,"path":"2019/05/05/failed-to-activate-lv/","link":"","permalink":"http://kristianreese.com/2019/05/05/failed-to-activate-lv/","excerpt":"","text":"Failed to activate new LVEDS etlprod2 ~ # lvcreate -l 14645 -n lv_dwstore vg_etlprod2 Not activating vg_etlprod2/lv_dwstore since it does not pass activation filter. Failed to activate new LV. ResolutionWithin my LVM configuration, I defined host tags by setting hosttags = 1 within the tags section as follows in /etc/lvm/lvm.conf. tags { hosttags = 1 } Because of this, I was unable to create a new logical volume within a volume group. Why is this? Because the volume group was not tagged. After tagging the volume group, I was then allowed to create a logical volume. EDS etlprod2 ~ # vgs -o vg_name,vg_tags vg_etlprod2 VG VG Tags vg_etlprod2 EDS etlprod2 ~ # vgchange --addtag etlprod2 vg_etlprod2 Volume group \"vg_etlprod2\" successfully changed EDS etlprod2 ~ # lvcreate -l 14645 -n lv_dwstore vg_etlprod2 Logical volume \"lv_dwstore\" created","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"directory index full","slug":"directory-index-full","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:00:43.000Z","comments":true,"path":"2019/05/05/directory-index-full/","link":"","permalink":"http://kristianreese.com/2019/05/05/directory-index-full/","excerpt":"","text":"How to handle “Directory index full” messages in syslogkernel: EXT3-fs warning (device dm-3): ext3_dx_add_entry: Directory index full! On systems in which this is being constantly logged to syslog, this can become rather annoying, but it is not a bug. It’s informing the user that somewhere within the system, a directory has more than 32,000 subdirectories. A directory can have at most 31998 subdirectories, because an inode can have at most 32000 links. (See external link below for source) Sometimes, this can be difficult to find, especially in a case whereby a physical server is hosting a couple of hundres VPS accounts. To help break down where the culprit(s) is coming from, I’ve put together this quick script to tell me where to start looking: #!/bin/bash# Kristian Reese# http://kristianreese.comfor veid in $(vzlist -aHh 0*.tld.com -o veid)do echo $veid vzctl exec $veid 'find /var -type d 2&gt;/dev/null | ( while read dir; do cnt=`ls -l \"$dir\" | wc -l` if [ \"$cnt\" -gt \"10000\" ]; then echo \"$cnt: $dir\" fi done )'echodone","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"postfix masquerade","slug":"postfix-masquerade","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:02:14.000Z","comments":true,"path":"2019/05/05/postfix-masquerade/","link":"","permalink":"http://kristianreese.com/2019/05/05/postfix-masquerade/","excerpt":"","text":"Setting relayhost in postfixThe relayhost parameter specifies the default host to send mail to. This is equivalent to the DS entry in sendmails MTA sendmail.cf config file. !!! note output below has been trucated for purposes of doucmentation: PROD linux ~ # cat /etc/postfix/main.cf# INTERNET OR INTRANET# The relayhost parameter specifies the default host to send mail to# when no entry is matched in the optional transport(5) table. When# no relayhost is given, mail is routed directly to the destination.## On an intranet, specify the organizational domain name. If your# internal DNS uses no MX records, specify the name of the intranet# gateway host instead.## In the case of SMTP, specify a domain, host, host:port, [host]:port,# [address] or [address]:port; the form [host] turns off MX lookups.## If you're connected via UUCP, see also the default_transport parameter.##relayhost = $mydomain#relayhost = [gateway.my.domain]#relayhost = [mailserver.isp.tld]#relayhost = uucphost#relayhost = [an.ip.add.ress]relayhost = mr.kristianreese.com If you’re on an internal network whose domain name does not resolve on the Internet or is not a valid Internet domain name, masquerading the outgoing SMTP email address may be necessary to ensure mail delivery. Many mail servers reject mail addresses with invalid domain names to avoid spam, so masquerading as something that does resolve can help. To accomplish this, postfix has a parameter smtp_generic_maps that can be setup in main.cf. If not present, append this to the config file: #specify lookup tables that replace local mail addresses with valid Internet addressessmtp_generic_maps = hash:/etc/postfix/generic Next, edit (or create if not present) /etc/postfix/generic and append: `@linuxmta.int.web.com linuxmta@networksolutions.com` Run the command postmap /etc/postfix/generic to create or update the postfix lookup tables, then restart postfix. This will set any email that is sent from linuxmta.int.web.com outbound as coming FROM `linuxmta@networksolutions.com`","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"},{"name":"postfix","slug":"postfix","permalink":"http://kristianreese.com/tags/postfix/"}],"author":"Kris Reese"},{"title":"Extracting initrd","slug":"initrd","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:01:20.000Z","comments":true,"path":"2019/05/05/initrd/","link":"","permalink":"http://kristianreese.com/2019/05/05/initrd/","excerpt":"","text":"How to extract and view contents of initrdDepending on your version of linux, the /boot/initrd may be one of two formats: cpio archive ext2 filesystem data The easiest way to determine what you’re working with is by using the file command. Here are examples showing the output for each file type listed in the above bullet points: [root@linux ~]# file /boot/initramfs-2.6.32-220.17.1.el6.x86_64.img /boot/initramfs-2.6.32-220.17.1.el6.x86_64.img: gzip compressed data, from Unix, last modified: Thu Jun 28 18:33:36 2012, max compression[root@linux ~]# file /boot/initramfs64-catalystinitramfs64-catalyst: gzip compressed data, from Unix, max compression To be sure what you’re dealing with, supply the -z option to the file command as this will attempt to look inside the compressed files to see what the contents are: [root@linux ~]# file -z /boot/initrd-2.6.9-023stab053.2-smp.img /boot/initrd-2.6.9-023stab053.2-smp.img: Linux rev 1.0 ext2 filesystem data (gzip compressed data, from Unix, max compression)[root@linux ~]# file -z /boot/initramfs64-catalystinitramfs64-catalyst: ASCII cpio archive (SVR4 with no CRC) (gzip compressed data, from Unix, max compression) Mount itMount ext2 filesystem data (gzip compressed) initrd contents I like to work with copies of the initrd, so I created a dir in /root/initrd and work with it there: [root@linux root]# mkdir /root/initrd; cd /root/initrd[root@linux initrd]# cp /boot/initrd-2.6.9-023stab053.2-smp.img . Some online documents indicate the initrd should be renamed with the .gz extension so that gunzip can be used to extract the compressed file, but most linux distros ship with the handy zcat command, which saves us a step from having to rename files. This is the method I prefer: [root@linux initrd]# zcat initrd-2.6.9-023stab053.2-smp.img &gt; initrd-2.6.9-023stab053.2-smp[root@linux initrd]# file -z initrd-2.6.9-023stab053.2-smpinitrd-2.6.9-023stab053.2-smp: Linux rev 1.0 ext2 filesystem data Now it’s ready to be mounted as a loop device: [root@linux initrd]# mount -o loop ./initrd-2.6.9-023stab053.2-smp /mnt[root@linux initrd]# ls -l /mnttotal 9drwxr-xr-x 2 root root 1024 Jun 25 15:40 bindrwxr-xr-x 2 root root 1024 Jun 25 15:40 devdrwxr-xr-x 4 root root 1024 Jun 25 15:40 etcdrwxr-xr-x 2 root root 1024 Jun 25 15:40 lib-rwxr-xr-x 1 root root 869 Jun 25 15:40 linuxrcdrwxr-xr-x 2 root root 1024 Jun 25 15:40 loopfsdrwxr-xr-x 2 root root 1024 Jun 25 15:40 proclrwxrwxrwx 1 root root 3 Jun 25 15:40 sbin -&gt; bindrwxr-xr-x 2 root root 1024 Jun 25 15:40 sysdrwxr-xr-x 2 root root 1024 Jun 25 15:40 sysroot Extract itExtract cpio archive initrd contents [root@linux initrd]# cp /boot/initramfs64-catalyst .[root@linux initrd]# zcat initramfs64-catalyst | cpio -id[root@vpsrep tmp]# ls -ltotal 387232drwxr-xr-x 2 root root 4096 Jun 26 21:57 bindrwxr-xr-x 2 root root 4096 Jun 26 21:57 bootdrwxr-xr-x 3 root root 4096 Jun 26 21:58 devdrwxr-xr-x 32 root root 4096 Jun 26 21:58 etcdrwxr-x--- 5 root root 4096 Jun 26 21:58 evolutiondrwxr-xr-x 2 root root 4096 Jun 26 21:58 homelrwxrwxrwx 1 root root 15 Jun 26 21:58 init -&gt; /evolution/init-rw-r--r-- 1 root root 396057600 Jun 26 21:55 initramfs64-catalyst.cpiolrwxrwxrwx 1 root root 5 Jun 26 21:58 lib -&gt; lib64drwxr-xr-x 2 root root 4096 Jun 26 21:58 lib32drwxr-xr-x 9 root root 4096 Jun 26 21:58 lib64drwxr-xr-x 2 root root 4096 Jun 26 21:58 mediadrwxr-xr-x 2 root root 4096 Jun 26 21:58 mntdrwxr-xr-x 2 root root 4096 Jun 26 21:58 optdrwxr-xr-x 2 root root 4096 Jun 26 21:58 procdrwx------ 2 root root 4096 Jun 26 21:58 rootdrwxr-xr-x 2 root root 4096 Jun 26 21:58 sbindrwxr-xr-x 2 root root 4096 Jun 26 21:58 sysdrwxrwxrwt 2 root root 4096 Jun 26 21:58 tmpdrwxr-xr-x 12 root root 4096 Jun 26 21:58 usrdrwxr-xr-x 9 root root 4096 Jun 26 21:58 var !!! note The --verbose option may be supplied to list the files to the console as they are being extracted, or adding v to the options in the example above, as in cpio -idv cpio -itv will simply list the contents without extracting them. The -itv options are short hand for –extract –list –verbose The -idv options are short hand for –extract –make-directories –verbose Repackaging initrd!!! tip “Repackaging” If some some reason a change needs to be made to the initrd and it needs to be buttoned back up, this is how to do it: ext2 filesystem datacopy whatever files you want to whereever the initrd ext2 filesystem was mounted. Once finished, umount the filesystem, gzip it (which will give it the .gz extension), then rename it to the original initrd file that was copied or some other name like initrd-2.6.9-023stab053.2-smp-mpath.img. If a new name is given, be sure to update the boot loader configuration files like /boot/grub/grub.conf. cpio archiveFrom within the directory the contents were extracted, first remove or move the .cpio file to another parent directory outside to ensure it’s not part of the new cpio archive then recreate the archive: find . | cpio --quiet -c -o | gzip -9 -n &gt; ../initramfs64-catalyst","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Redirecting stdout and stderr","slug":"redirecting-stdout-and-stderr","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:02:27.000Z","comments":true,"path":"2019/05/05/redirecting-stdout-and-stderr/","link":"","permalink":"http://kristianreese.com/2019/05/05/redirecting-stdout-and-stderr/","excerpt":"","text":"How to redirect standard output and standard errorWe’ve seen the numbers used in redirecting stdout and stderr. So what do they mean? 1 represents standard output 2 represents standard error 2&gt;&amp;1 indicates that the standard error (2) is redirected (&gt;) to the same file descriptor (&amp;1) that is pointed by standard output (1) stdoutredirect standard output to /dev/null and redirect standard error to standard output (meaning all output to both standard output/error are redirected to /dev/null) &gt;/dev/null 2&gt;&amp;1 stderrsend all error messages to /dev/null 2&gt;/dev/null","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Setup Local Mirror of RHEL Repo","slug":"setup-local-mirror-rhel-repo","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:03:19.000Z","comments":true,"path":"2019/05/05/setup-local-mirror-rhel-repo/","link":"","permalink":"http://kristianreese.com/2019/05/05/setup-local-mirror-rhel-repo/","excerpt":"","text":"How To create your own local Red Hat Enterprise Linux yum repository serverTo begin, build a virutal machine or stand alone system installing the same OS version of Red Hat you wish to serve as the yum repository. At the time of this writing, my base OS plus mirror data consumed 14G of disk space so be sure to size your disk appropriately. This is necessary as you must register the mirror server to a RHN Satellite, which allows the system to receive software updates. It’s also important to know that one physical system cannot subscribe to multiple architecture base channels. In otherwords, if you install a 64-bit OS, you cannot subscribe to the 32-bit channel. After the system has been deployed, you’re ready to begin: Register with RHN~# rhn_register -vv --nox Install required packaged~# yum install createrepo~# yum install yum-utils Create a directory for the repo and sync the base channel~# mkdir /opt/rhel6repo~# reposync --gpgcheck -l --repoid=rhel-x86_64-server-6 --download_path=/opt/rhel6repo !!! note This will take a long time to run. I ran mine overnight Create repo data~# createrepo /opt/rhel6repo Configure apache to serve the content~# cat /etc/httpd/conf.d/rhel-6-repo.confAlias /rhel-6-repo /opt/rhel6repo&lt;Directory /opt/repository&gt; Options Indexes MultiViews FollowSymLinks Order allow,deny Allow from all&lt;/Directory&gt; Start/restart apache~# service httpd start Configure the clientsAdd the repo to the yum config: ~# cat /etc/yum.repos.d/rhel6.repo[rhel-6-repo]name=My Red Hat Enterprise Linux $releasever - $basearchbaseurl=http://&lt;IP_ADDRESS&gt;/rhel-6-repoenabled=1gpgcheck=0 From the client, verify it can see the mirror with: yum repolist !!! info You can disable connectivity to Red Hat network by changing the value of enabled = 0 in the file /etc/yum/pluginconf.d/rhnplugin.conf Clean out the yum cache and remove old header info on the client. ~# yum clean all Now, you can list all of the updates available from your local mirror. ~# yum list updates","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Sorting IP Addresses via bash","slug":"sort-ip-addresses","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T02:57:27.000Z","comments":true,"path":"2019/05/05/sort-ip-addresses/","link":"","permalink":"http://kristianreese.com/2019/05/05/sort-ip-addresses/","excerpt":"","text":"Sort list of IP addresses via command lineDo you have a text file filled with a bunch of IP addresses, and it won’t sort properly with the -n option alone? Providing additional options can help achive the desired output. $ cat nfs_clients | sort -n -t . -k 1,1 -k 2,2 -k 3,3 -k 4,4 A description of the additional options from the sort man page: -k, --key=POS1[,POS2] start a key at POS1 (origin 1), end it at POS2 (default end of line)-t, --field-separator=SEP use SEP instead of non-blank to blank transition From the command line example above, we’ve indicated the ‘dot’ or ‘period’ to be the field-separator and have defined keys in each of the IPs octets.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"How To Check If LFS is Enabled","slug":"how-to-check-if-Large-file-Size-is-enabled","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:01:06.000Z","comments":true,"path":"2019/05/05/how-to-check-if-Large-file-Size-is-enabled/","link":"","permalink":"http://kristianreese.com/2019/05/05/how-to-check-if-Large-file-Size-is-enabled/","excerpt":"","text":"To check if a file system supports the LFS (Large File System) standard, you can use the getconf command. If the result is 64, LFS is supported. getconf FILESIZEBITS / In the example above, the root file system was checked. If /var for example, is a separate file system, specify /var in its place. You may also use the mount command to verify the file system type that is being used. ext2 / ext3, for example, has full support for LFS. I found this nice read take from suse.de","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"ip_conntrack table full","slug":"ip-conntrack","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T03:01:32.000Z","comments":true,"path":"2019/05/05/ip-conntrack/","link":"","permalink":"http://kristianreese.com/2019/05/05/ip-conntrack/","excerpt":"","text":"ip_conntrack: table full, dropping packetAt one point, there was high call volume into our support center of customers complaining about severe lag. One common denominator was that the customer base who called in happened to all reside on the same server, so investigation into the matter focused on that one particular system. The server’s load average was really low, and had plenty of free RAM, though connectivity to customers hosted websites were lagging. After running dmesg, I noticed “ip_conntrack: table full, dropping packet”. After observing netstat -an for a bit, it was clear the server was being used to send SPAM. After blocking the connections and securing the customer SMTP passwords, the counts came down and the lag ceased. The following command can be used to see what the max setting is for this kernel parameter: /sbin/sysctl net.ipv4.ip_conntrack_max or cat /proc/sys/net/ipv4/ip_conntrack_max To see how many you are using at present: wc -l /proc/net/ip_conntrack or cat /proc/sys/net/ipv4/netfilter/ip_conntrack_count The setting can be adjusted, and if to be made permanent, make the change in /etc/sysctl.conf. In this example, the max setting is increased to 65535. echo \"net.ipv4.ip_conntrack_max = 65535\" &gt; /etc/sysctl.conf/sbin/sysctl -w To increase it temporarily (non-persistent across reboots) echo 131072 &gt; /proc/sys/net/ipv4/ip_conntrack_max","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Setup yum repo from DVD ISO","slug":"setup-yum-dvdiso-repo","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-05T02:57:51.000Z","comments":true,"path":"2019/05/05/setup-yum-dvdiso-repo/","link":"","permalink":"http://kristianreese.com/2019/05/05/setup-yum-dvdiso-repo/","excerpt":"","text":"How to set up a yum repo from locally mounted DVD ISOMount the iso to /mnt In the case of redhat, copy the media.repo file from the root dir of the mount point to /etc/yum.repos.d. Edit the file and add the baseurl at the end as depicted below. [root@mkoffer2 ~]# cat /etc/yum.repos.d/media.repo [InstallMedia]name=Red Hat Enterprise Linux 6.3mediaid=1339640147.274118metadata_expire=-1gpgcheck=0cost=500baseurl=file:///mnt !!! tip mediaid comes from the .diskinfo file located in the root of the DVD This will allow installation of base packages. If you need non-base packages, you’ll need to include the correct stanza in the media.repo config file. Refer: redhat knowledgebase","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Advanced techniques using unix 'find'","slug":"advanced-techniques-for-using-the-unix-find-command","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T01:45:21.000Z","comments":true,"path":"2019/05/05/advanced-techniques-for-using-the-unix-find-command/","link":"","permalink":"http://kristianreese.com/2019/05/05/advanced-techniques-for-using-the-unix-find-command/","excerpt":"","text":"I found this article on IBMs Developer Works site and had to put it here! Some of this is basic, but it’s a good read nonetheless. There’s nothing quite like the thrill of exploring, discovering new people, places, and things. The territory might change, but a few principles remain the same. One of those principles is to keep a written record of your journey; another is to know and use your tools. The UNIX® operating system is much like a vast, uncharted wilderness. As you travel the terrain, you can pick up tools that assist you later. The find command is such a tool. The find command is capable of much more than simply locating files; it can automatically execute sequences of other UNIX commands, using the filenames found for input, as this article explains. Find with few limitsAll operating systems worth their salt have a tool to assist you in finding things. Unlike most of these tools, the UNIX find command can automatically perform many operations for you on the files it finds. Standard find tools found in graphical user interfaces (GUIs) allow you to do a few common tasks with the files you find: You can mark them for cutting, copying, and pasting; you can move them to a new location; and you can open them with the program used to create them. These operations involve two or more steps and aren’t automatic – you find the files first, and then you use the GUI to mark them for the next operation. This approach is fine for many users, but the explorer wants more. The UNIX find command can delete, copy, move, and execute files that it finds. In addition, with the -exec parameter, it can automatically run files through any sequence of UNIX commands you need. It can even ask you before it performs such operations on any file. Simplify management of your file systemThe UNIX find command, like most UNIX commands, has an intimidating array of options and switches that can discourage people from learning its depth – but true explorers aren’t intimidated just because the territory is vast. A good general principle goes a long way toward simplifying a complex topic. Start up an xterm, and try the following command: $ find . -name *.gif -exec ls {} \\; The -exec parameter holds the real power. When a file is found that matches the search criteria, the -exec parameter defines what to do with the file. This example tells the computer to: Search from the current directory on down, using the dot (.) just after find. Locate all files that have a name ending in .gif (graphic files). List all found files, using the ls command. The -exec parameter requires further scrutiny. When a filename is found that matches the search criteria, the find command executes the ls {} string, substituting the filename and path for the {} text. If saturn.gif was found in the search, find would execute this command: $ ls ./gif_files/space/solar_system/saturn.gif The rest of the article builds on this general principle: Thoughtful use of the find command can make the management of UNIX file systems a much easier task. For example, the find command can process commands based on the type of file system where the file is found, if you use the -fstype parameter. And it’s often useful to have the find command prompt you before it executes commands on a found file; you can tell it to do so by using the -ok parameter, as you’ll see next. Optional executionAn important alternative to the -exec parameter is -ok; it behaves the same as -exec, but it prompts you to see if you want to run the command on that file. Suppose you want to remove most of the .txt files in your home directory, but you wish to do it on a file-by-file basis. Delete operations like the UNIX rm command are dangerous, because it’s possible to inadvertently delete files that are important when they’re found by an automated process like find; you might want to scrutinize all the files the system finds before removing them. The following command lists all the .txt files in your home directory. To delete the files, you must enter Y or y when the find command prompts you for action by listing the filename: $ find $HOME/. -name *.txt -ok rm {} \\; Each file found is listed, and the system pauses for you to enter Y or y. If you press the Enter key, the system won’t delete the file. Listing 1 shows some sample results: Listing 1. Sample results &lt; rm ... /home/bill/./.kde/share/apps/karm/karmdata.txt &gt; ?&lt; rm ... /home/bill/./archives/LDDS.txt &gt; ?&lt; rm ... /home/bill/./www/txt/textfile1.txt &gt; ?&lt; rm ... /home/bill/./www/txt/faq.txt &gt; ?&lt; rm ... /home/bill/./www/programs/MIKE.txt &gt; ?&lt; rm ... /home/bill/./www/programs/EESTRING.txt &gt; ?... After each question mark, the system paused; in this case, the Enter key was pressed to continue to the next file. (No files were removed.) The -ok parameter lets you control the automatic processing of each found file, adding a measure of safety to the danger of automatic file removal. If too many files are involved for you to spend time with the -ok parameter, a good rule of thumb is to run the find command with -exec to list the files that would be deleted; then, after examining the list to be sure no important files will be deleted, run the command again, replacing ls with rm. Both -exec and -ok are useful, and you must decide which works best for you in your current situation. Remember, safety first! Use find creativelyYou can perform myriad tasks with the find command. This section provides some examples of ways you can put find to work as you manage your file system. To keep things simple, these examples avoid -exec commands that involve the piping of output from one command to another. However, you’re free to use commands like these in a find’s -exec clause. Clean out temporary filesYou can use find to clean directories and subdirectories of the temporary files generated during normal use, thereby saving disk space. To do so, use the following command: $ find . \\( -name a.out -o -name &#39;*.o&#39; -o -name &#39;core&#39; \\) -exec rm {} \\; File masks identifying the file types to be removed are located between the parentheses; each file mask is preceded by -name. This list can be extended to include any temporary file types you can come up with that need to be cleaned off the system. In the course of compiling and linking code, programmers and their tools generate file types like those shown in the example: a.out, *.o, and core. Other users have similar commonly generated temporary files and can edit the command accordingly, using file masks like *.tmp, *.junk, and so on. You might also find it useful to put the command into a script called clean, which you can execute whenever you need to clean a directory. Copy a directory’s contentsThe find command lets you copy the entire contents of a directory while preserving the permissions, times, and ownership of every file and subdirectory. To do so, combine find and the cpio command, like this: Listing 2. Combining the find and cpio command $ cd /path/to/source/dir$$ find . | cpio -pdumv /path/to/destination/dir The cpio command is a copy command designed to copy files into and out of a cpio or tar archive, automatically preserving permissions, times, and ownership of files and subdirectories. List the first lines of text filesSome people use the first line of every text file as a heading or description of the file’s contents. A report that lists the filenames and first line of each text file can make sifting through several hundred text files a lot easier. The following command lists the first line in every text file in your home directory in a report, ready to be examined at your leisure with the less command: Listing 3. The less command $ find $HOME/. -name *.txt -exec head -n 1 -v &#123;&#125; \\; &gt; report.txt$$ less &lt; report.txt Maintain LOG and TMP file storage spacesTo maintain LOG and TMP file storage space for applications that generate a lot of these files, you can put the following commands into a cron job that runs daily: Listing 4. Maintaining LOG and TMP file storage spaces $ find $LOGDIR -type d -mtime +0 -exec compress -r &#123;&#125; \\;$$ find $LOGDIR -type d -mtime +5 -exec rm -f &#123;&#125; \\; The first command runs all the directories (-type d) found in the $LOGDIR directory wherein a file’s data has been modified within the last 24 hours (-mtime +0) and compresses them (compress -r {}) to save disk space. The second command deletes them (rm -f {}) if they are more than a work-week old (-mtime +5), to increase the free space on the disk. In this way, the cron job automatically keeps the directories for a window of time that you specify. Copy complex directory treesIf you want to copy complex directory trees from one machine to another while preserving copy permissions and the User ID and Group ID (UID and GID – numbers used by the operating system to mark files for ownership purposes), and leaving user files alone, find and cpio once again come to the rescue: Listing 5. Maintaining LOG and TMP file storage spaces $ cd /source/directory$$ find . -depth -print | cpio -o -O /target/directory Find links that point to nothingTo find links that point to nothing, use the perl interpreter with find, like this: $ find / -type l -print | perl -nle &#39;-e || print&#39;; or loop through:$ for f in `find . -type l ! -exec test -r &#123;&#125; \\; -print`; do rm -f $f; done or use the built in -L option: $ find -L -type l -exec rm -f {} \\; This command starts at the topmost directory (/) and lists all links (-type l -print) that the perl interpreter determines point to nothing (-nle &#39;-e || print&#39;) – see the Resources section for more information regarding this tip from the Unix Guru Universe site. You can further pipe the output through the rm -f {} functionality if you want to delete the files. Perl is, of course, one of the many powerful interpretive language tools also found in most UNIX toolkits. Locate and rename unprintable directoriesIt’s possible in UNIX for an errant or malicious program to create a directory with unprintable characters. Locating and renaming these directories makes it easier to examine and remove them. To do so, you first include the -i switch of ls to get the directory’s inode number. Then, use find to turn the inode number into a filename that can be renamed with the mv command: Listing 6. Locating and renaming unprintable directories $ ls -ail$$ find . -inum 211028 -exec mv &#123;&#125; newname.dir \\; List zero-length files To list all zero-length files, use this command: $ find . -empty -exec ls {} \\; After finding empty files, you might choose to delete them by replacing the ls command with the rm command. Find all php files and tar them up: find . -name *.php -exec tar -uvf ~/www/allphp.tar {} \\; grep for “base64_decode” in files find . -type f -exec grep -l &#39;base64_decode&#39; {} \\; Search for files 5 MB or larger, then list its size and location from smallest to largest in size. Use the -mount option to instruct find not to descend directories on other file systems. find / -mount -type f -size +5096k -exec ls -lh {} \\; | awk &#39;{ print $5 &quot;: &quot; $9 }&#39; | sort -n Search for files based on a certain time frame and perform some action. For example, if a file system is filling up due to a bunch of large log files, and the option to delete them is not available, search for and zip them up: find * -type f -mtime +4 -print -exec /usr/local/bin/gzip --best {} \\; or look for files older than 7 days that are NOT named *.gz and zip them the up: find . -mtime +7 ! -name &quot;*.gz&quot; -exec gzip {} \\; To apply tests only at certain depths, the depth, mindepth, and maxdepth can be used to aid in finding directories that are consuming the most space. find /var -depth -mindepth 1 -maxdepth 3 -type d -exec du -sh {} \\; Clearly, your use of the UNIX find command is limited only by your knowledge and creativity. Conclusion Exploring the vast terrain of the UNIX file system is easy with the find command. It is well worth your time to experiment with this command and see what works for you. As shown in the examples listed in this article, you can use find in many creative ways to make the exploration and management of your file system terrain easy, and fun. Bill Zimmerly is a knowledge engineer, a low-level systems programmer with expertise in various versions of UNIX and Microsoft® Windows®, and a free thinker who worships at the altar of Logic. Bill is also known as an unreasonable person. Unreasonable as in, “Reasonable people adapt themselves to the world. Unreasonable people attempt to adapt the world to themselves. All progress, therefore, depends on unreasonable people” (George Bernard Shaw). Creating new technologies and writing about them are his passions. He resides in rural Hillsboro, Missouri, where the air is fresh, the views are inspiring, and good wineries are all around. There’s nothing quite like writing an article on UNIX shell scripting while sipping on a crystal-clear glass of Stone Hill Blush. You can contact him at bill@zimmerly.com.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Resize an iscsi LUN in Linux","slug":"resize-iscsi-lun","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T01:54:48.000Z","comments":true,"path":"2019/05/05/resize-iscsi-lun/","link":"","permalink":"http://kristianreese.com/2019/05/05/resize-iscsi-lun/","excerpt":"","text":"How To Resize an iscsi LUN in LinuxNetApp storage systems also allow you to resize a LUN dynamically; however, the iSCSI layer in Linux is not capable of detecting the change in the LUN size. To display the new size of the LUN you must restart the iSCSI service. But a restart of the iSCSI service fails if any iSCSI devices are mounted at that time. Because of this, even if the LUN on which the file system is located is resized, you must unmount the file system and then restart iSCSI and the multipathing service before Linux can detect the new size. !!! warning Though this procedure below has been tested, it is always a good idea to back up your data before attempting resizing. Collect information on the partition to be resizedFor the purposes of this example, /dev/sdc1 is the iscsi lun partition to be resized. Run through the following commands and set the information aside: ~# df /dev/sdc1~# df -h /dev/sdc1~# fdisk -l /dev/sdc~# fdisk -s /dev/sdc1 !!! info The first two commands are to provide a “snapshot” look at the size of the partition before making any changes. The third command contains useful information needed to successfully increase the partition in a later step. Make note of the starting cylinder from the output. For example: PRODUCTION linux01 # fdisk -l /dev/sdc Disk /dev/sdc: 32.2 GB, 32212254720 bytes64 heads, 32 sectors/track, 30720 cylindersUnits = cylinders of 2048 * 512 = 1048576 bytes Device Boot Start End Blocks Id System/dev/sdc1 1 30720 31457264 83 Linux !!! tip Note the amount of cylinders and what the starting cylinder is. In the example above, the starting cylinder is 1. !!! info If the partition to be increased contains system/OS files, you may need to boot off a rescue disk to perform the resize operation. Otherwise, it the parition is a data partition, then it simply needs to be unmounted. Increase size of the LUN on the NetAppnetapp&gt; lun resize /vol/mylun +5g Unmount the devicePRODUCTION linux01 # umount /dev/sdc1 Perform a file system checkPRODUCTION linux01 # fsck -n /dev/sdc1 Restart iscsi daemonPRODUCTION linux01 # /etc/init.d/iscsid restart * Unmounting iscsi partition... ... * Logging out of iSCSI targets * Logging out of session [sid: 1, target: iqn.1992-08.com.netapp:sn.151753609, portal: 10.10.2.2,3260] * Logout of [sid: 1, target: iqn.1992-08.com.netapp:sn.151753609, portal: 10.10.2.2,3260]: successful * Stopping iSCSI initiator service ... [ ok ] * Removing iSCSI modules ... [ ok ] * Loading iSCSI modules ... [ ok ] * Starting iSCSI initiator service ... [ ok ] * Logging into iSCSI targets * Logging in to [iface: default, target: iqn.1992-08.com.netapp:sn.151753609, portal: 10.10.2.2,3260] * Login to [iface: default, target: iqn.1992-08.com.netapp:sn.151753609, portal: 10.10.2.2,3260]: successful * Waiting for udev to process new devices * Mounting iscsi partition... ... !!! note From the ‘fdisk -l’ output below, the device changed from /dev/sdc to /dev/sdb. This may happen due to udev during iscsi restart and is normal. You will also notice the amount of cylinders has changed due to the lun size increase command performed on the netapp but the cylidner start/end is the same as before. PRODUCTION linux01 # fdisk -l&lt;snip&gt;Disk /dev/sdb: 107.3 GB, 107374182400 bytes64 heads, 32 sectors/track, 102400 cylindersUnits = cylinders of 2048 * 512 = 1048576 bytes Device Boot Start End Blocks Id System /dev/sdb1 1 30720 31457264 83 Linux Run a file system checkEnsure the file system is clean before attempting to change the layout of the partition. PRODUCTION linux01 # fsck -n /dev/sdb1fsck 1.38 (30-Jun-2005)e2fsck 1.38 (30-Jun-2005)/dev/sdb1 has gone 701 days without being checked, check forced.Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivity/lost+found not found. Create? noPass 4: Checking reference countsPass 5: Checking group summary information/dev/sdb1: ********** WARNING: Filesystem still has errors **********/dev/sdb1: 16940/3932160 files (0.3% non-contiguous), 7425413/7864316 blocks It still has errors because lost+found directory is missing in ==MY== example, but I opted not to create it in this step to avoid further changes to the file system. (optional) Convert the file system into an ext2 file systemThis may be optional. Depending on your kernel, resize2fs may only work on ext2 file systems. Refer to your man page. In my case, it doesn’t support ext3 so the steps involved are to: remove the journal PRODUCTION linux01 # tune2fs -O ^has_journal /dev/sdb1tune2fs 1.38 (30-Jun-2005) delete the current partition table and recreate it with the new cylinder boundaries PRODUCTION linux01 # fdisk /dev/sdb The number of cylinders for this disk is set to 102400.There is nothing wrong with that, but this is larger than 1024,and could in certain setups cause problems with:1) software that runs at boot time (e.g., old versions of LILO)2) booting and partitioning software from other OSs (e.g., DOS FDISK, OS/2 FDISK) Command (m for help): p Disk /dev/sdb: 107.3 GB, 107374182400 bytes64 heads, 32 sectors/track, 102400 cylindersUnits = cylinders of 2048 * 512 = 1048576 bytes Device Boot Start End Blocks Id System/dev/sdb1 1 30720 31457264 83 Linux Command (m for help): d Selected partition 1 Command (m for help): p Disk /dev/sdb: 107.3 GB, 107374182400 bytes64 heads, 32 sectors/track, 102400 cylindersUnits = cylinders of 2048 * 512 = 1048576 bytes Device Boot Start End Blocks Id System Command (m for help): n Command action e extended p primary partition (1-4) p Partition number (1-4): 1 First cylinder (1-102400, default 1): 1 &lt;- taken from fdisk -l output in preliminary steps. Make sure start cylinder is same as it was before!!! If you get an error like \"Value out of range\", you may need to use parted instead of fdisk. Se footnotes below.Last cylinder or +size or +sizeM or +sizeK (1-102400, default 102400):Using default value 102400 Command (m for help): p Disk /dev/sdb: 107.3 GB, 107374182400 bytes64 heads, 32 sectors/track, 102400 cylindersUnits = cylinders of 2048 * 512 = 1048576 bytes Device Boot Start End Blocks Id System/dev/sdb1 1 102400 104857584 83 Linux Command (m for help): w The partition table has been altered! Calling ioctl() to re-read partition table.Syncing disks. Run another file system check. Note this time we use e2fsck because we’re still an ext2 file system. This time, I created the lost+found. PRODUCTION linux01 # e2fsck -f /dev/sdb1e2fsck 1.38 (30-Jun-2005)Pass 1: Checking inodes, blocks, and sizesPass 2: Checking directory structurePass 3: Checking directory connectivity/lost+found not found. Create&lt;y&gt;? yesPass 4: Checking reference countsPass 5: Checking group summary information/dev/sdb1: ***** FILE SYSTEM WAS MODIFIED *****/dev/sdb1: 16941/3932160 files (0.3% non-contiguous), 7392612/7864316 blocks Resize the partitionPRODUCTION linux01 # resize2fs /dev/sdb1resize2fs 1.38 (30-Jun-2005)Resizing the filesystem on /dev/sdb1 to 26214396 (4k) blocks.The filesystem on /dev/sdb1 is now 26214396 blocks long. Run another fsckEnsure it is clean by running another file system check: PRODUCTION linux01 # fsck -n /dev/sdb1fsck 1.38 (30-Jun-2005)e2fsck 1.38 (30-Jun-2005)/dev/sdb1: clean, 16941/13107200 files, 7680539/26214396 blocks The resize operation is now complete. Add an ext3 journal to the filesystemPRODUCTION linux01 # tune2fs -j /dev/sdb1tune2fs 1.38 (30-Jun-2005)Creating journal inode: doneThis filesystem will be automatically checked every 25 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override. Remount the lunPRODUCTION linux01 # mount /dev/sdb1 /mnt Enjoy the new size!PRODUCTION linux01 # df -h /dev/sdb1Filesystem Size Used Avail Use% Mounted on/dev/sdb1 99G 28G 67G 30% /var/services/db/mysqlPRODUCTION linux01 # PRODUCTION linux01 # ls -l /mnttotal 28853216drwx------ 2 mysql mysql 4096 Jul 30 22:47 database22_100drwx------ 2 mysql mysql 4096 Jul 30 22:47 database22_101drwx------ 2 mysql users 4096 Jul 30 22:47 database22_102drwx------ 2 mysql users 4096 Jul 30 22:47 database22_103drwx------ 2 mysql users 4096 Jul 30 22:47 database22_104drwx------ 2 mysql users 4096 Jul 30 22:47 database22_105drwx------ 2 mysql users 4096 Jul 30 22:47 database22_106drwx------ 2 mysql users 4096 Jul 30 22:47 database22_107...... FootnotesIn this example, fdisk supplied an error “Value out of range” when trying to set a starting cyclinger to a value less than 2048. I had an incident where a LUNs starting cyclinder was 63, and since I did not want to lose the data held between data 63 and 2048, I used parted to repartition the disk. Having a starting cyclinder of 63 more than likely means not be aligned with the underlying storage, but that’s another topic. There may be better ways to do this, but this is how I’ve worked it out and am documenting it until I have the opportunity to run through the process again. If improvements can be made, I’ll update the doc or will review anyone who cares to comment. PRODUCTION linux101 ~ # parted /dev/sdbGNU Parted 3.1Using /dev/sdbWelcome to GNU Parted! Type 'help' to view a list of commands.(parted) print Model: NETAPP LUN (scsi)Disk /dev/sdb: 107GBSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 1049kB 107GB 107GB primary(parted) unit cyl (parted) print Model: NETAPP LUN (scsi)Disk /dev/sdb: 13055cylSector size (logical/physical): 512B/512BBIOS cylinder,head,sector geometry: 13055,255,63. Each cylinder is 8225kB.Partition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 0cyl 13054cyl 13054cyl primary(parted) unit s (you might have to type 'unit &lt;enter&gt; s')(parted) print Model: NETAPP LUN (scsi)Disk /dev/sdb: 209743872sSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 2048s 209717247s 209715200s primary(parted) rm 1 (parted) print Model: NETAPP LUN (scsi)Disk /dev/sdb: 209743872sSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags(parted) mkpart Partition type? primary/extended? primary File system type? [ext2]? ext3 Start? 63s End? 209717247s &lt;&lt;&lt; use value from output in print above in matching highlight (see footnote below) Warning: The resulting partition is not properly aligned for best performance.Ignore/Cancel? Ignore (parted) print Model: NETAPP LUN (scsi)Disk /dev/sdb: 209743872sSector size (logical/physical): 512B/512BPartition Table: msdosDisk Flags: Number Start End Size Type File system Flags1 63s 209717247s 209717185s primary ext2(parted) quit Information: You may need to update /etc/fstab.PRODUCTION atl4mysqlv101 ~ # fdisk -l Disk /dev/sda: 292.3 GB, 292326211584 bytes255 heads, 63 sectors/track, 35539 cylinders, total 570949632 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x77e3ed41 Device Boot Start End Blocks Id System/dev/sda1 63 8016434 4008186 83 Linux/dev/sda2 8016435 16032869 4008217+ 83 Linux/dev/sda3 16032870 20049119 2008125 82 Linux swap / Solaris/dev/sda4 20049120 570934034 275442457+ 5 Extended/dev/sda5 20049183 36065924 8008371 83 Linux/dev/sda6 36065988 52082729 8008371 83 Linux/dev/sda7 52082793 570934034 259425621 83 LinuxDisk /dev/sdb: 107.4 GB, 107388862464 bytes255 heads, 63 sectors/track, 13055 cylinders, total 209743872 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x0005ed03 Device Boot Start End Blocks Id System/dev/sdb1 63 209717247 104858592+ 83 LinuxPRODUCTION mysqlv101 ~ # Footnotes: If the end sector matches what it was previous, I’ve had to use fdisk just to see what the End value is of the extended LUN, then use parted to ensure the use of the correct starting cyl.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"How To Rescan Linux for a New LUN","slug":"rescan-for-lun","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T01:46:52.000Z","comments":true,"path":"2019/05/05/rescan-for-lun/","link":"","permalink":"http://kristianreese.com/2019/05/05/rescan-for-lun/","excerpt":"","text":"How to rescan Linux for newly presented LUNsThis article focuses on utilizing the lun_scan utility provided by Emulex No-Reboot Dynamic Target/LUN Discovery Tool. This tool is part of the Emulex Drivers for Linux. To perform scans manually, the following set of commands may be issued. # ls -1 /sys/class/fc_hosttotal 0drwxr-xr-x 3 root root 0 Jul 9 02:37 host0drwxr-xr-x 3 root root 0 Jul 9 02:37 host1 # echo# echo &quot;1&quot; &gt; /sys/class/fc_host/host1/issue_lip# echo &quot;1&quot; &gt; /sys/class/fc_host/host2/issue_lip# echo &quot;- - -&quot; &gt; /sys/class/scsi_host/host1/scan# echo &quot;- - -&quot; &gt; /sys/class/scsi_host/host2/scan TIP You can also run this through a for loop for host in $(ls -1d /sys/class/fc_host/*); do echo \"1\" &gt; $&#123;host&#125;/issue_lip; done for host in $(ls -1d /sys/class/scsi_host/*); do echo \"- - -\" &gt; $&#123;host&#125;/scan ; done After running the commands which perform a rescan, you may see if any newly discovered LUNs are now part of the system by looking at: cat /proc/scsi/scsi Once there, bring them into linux dm-multipath by issuing: multipath -v1multipath -v2 The multipath -v2 command prints out multipathed paths that show which devices are multipathed. If the command does not print anything out, ensure that all SAN connections are set up properly and the system is multipathed. multipath -ll Here are my cliff notes from when I installed a pair of Emulex HBAs, created LUNs on the EMC and brought them into Linux. I’ve getting ready to go on vacation, so I’m placing my notes out here verbatim with the intention of coming back to format these into a more knowledge sharing format. Therefore, these are my notes and may not be very descriptive for the purposes of knowlede sharing. If there are any questions, feel free to contact me. How to present EMC LUNs to RHEL using dm-multipath perform zoning using connectrix install HostAgent on RHEL system and start it up In Navisphere: Create a new storage group right click on the newly created storage group and Connect Hosts Select LUNs yum install device-mapper-multipath device-mapper-multipath-libs multipath-tools dm-devel pciutils kernel-devel make gcc rpm-build redhat-rpm-config EDS etlprod2 ~ # multipath -v3 -dMar 28 15:57:51 | DM multipath kernel driver not loadedMar 28 15:57:51 | DM multipath kernel driver not loadedEDS etlprod2 ~ # chkconfig multipathd onEDS etlprod2 ~ # /etc/init.d/multipathd startEDS etlprod2 ~ # multipath -v3 -dget wwid of sda device and change multipath.conf todefaults &#123; user_friendly_names yes path_grouping_policy multibus path_checker emc_clariion hwtable_regex_match yes&#125;blacklist &#123; devnode &quot;^(ram|raw|loop|fd|md|dm-|sr|scd|st)[0-9]*&quot; devnode &quot;^cciss!c[0-9]d[0-9]*&quot; devnode &quot;^sda[[0-9]*]&quot; wwid 36782bcb02255b5001895ca3f07c57353&#125;devices &#123; device &#123; vendor &quot;DGC&quot; product &quot;.*&quot; product_blacklist &quot;LUNZ&quot; getuid_callout &quot;/lib/udev/scsi_id --whitelisted --device=/dev/%n&quot; #prio_callout &quot;/sbin/mpath_prio_emc /dev/%n&quot; features &quot;1 queue_if_no_path&quot; hardware_handler &quot;1 emc&quot; path_selector &quot;round-robin 0&quot; path_grouping_policy group_by_prio failback immediate rr_weight uniform no_path_retry 60 rr_min_io 1000 path_checker emc_clariion prio emc &#125;&#125; Run lun_scan.sh multipath -ll Right click on Host in storage group and choose “Update Now” This will put the device name at the end of each LUN mpath are in /dev/mapper !!! tip “create aliases flat file” EDS etlprod2 emc # cat aliases LUN 24 etlprod2 metabackupvg-backup2lv RAID Group 13 70 GB mpatho LUN 25 etlprod2 metabackupvg-backuplv RAID Group 13 45 GB mpathe LUN 26 etlprod2 metadatavg-data01lv RAID Group 12 11 GB mpathf LUN 27 etlprod2 metadatavg-data02lv RAID Group 12 11 GB mpathh LUN 28 etlprod2 metadatavg-data03lv RAID Group 12 11 GB mpathi LUN 29 etlprod2 metadatavg-data04lv RAID Group 12 11 GB mpathn LUN 30 etlprod2 metadatavg-data05lv RAID Group 11 20 GB mpathk LUN 31 etlprod2 metadatavg-data06lv RAID Group 11 20 GB mpathg LUN 32 etlprod2 metadatavg-data07lv RAID Group 11 14 GB mpathc LUN 33 etlprod2 metalogvg-arch2lv RAID Group 10 14 GB mpathj LUN 34 etlprod2 metalogvg-archlv RAID Group 10 5 GB mpathl LUN 35 etlprod2 metalogvg-log01lv RAID Group 10 3 GB mpathd LUN 36 etlprod2 metalogvg-log02lv RAID Group 10 3 GB mpathb LUN 37 etlprod2 srcdata2vg-srcdata2lv RAID Group 9 50 GB mpathm Create a script to parse the above aliases fileEDS etlprod2 emc # cat map_wwid_to_lun.sh #!/bin/bashecho &quot;multipaths &#123;&quot;for device in $(cat aliases | awk &apos;&#123; print $10 &#125;&apos;)do wwid=`scsi_id -g /dev/mapper/$device` alias=`grep $device aliases | awk &apos;&#123; print $4 &#125;&apos;` #multipath -ll | grep &quot;$wwid&quot; echo &quot; multipath &#123; wwid $wwid alias $alias &#125;&quot;doneecho &quot;&#125;&quot; Add the output to multipath.conf, and run the following commands:multipath reloadmultipath -ll Use LVM to create Logical Volumes and Volume GroupsMake sure set filter exists in the lvm config file lvm.conf set filter FDISK!!! note set starting block at 128 for EMC EDS etlprod2 ~ # FDISK_CMDLIST=\"n\\np\\n1\\n\\n\\nt\\n8e\\nx\\nb\\n1\\n128\\nw\\n\"EDS etlprod2 ~ # for device in $(lvmdiskscan | grep mapper | awk '&#123; print $1 &#125;' | sort -n); do echo -e -n \"$&#123;FDISK_CMDLIST&#125;\" | ( fdisk $device ); done PVCREATE!!! quote “Example” pvcreate /dev/mapper/auditlogvg-log0[123]p1) EDS etlprod2 ~ # for device in $(lvmdiskscan | grep p1 | awk '&#123; print $1 &#125;' | sort -n); do pvcreate $device; done Physical volume \"/dev/mapper/metabackup-backup01p1\" successfully created Physical volume \"/dev/mapper/metabackup-backup02p1\" successfully created Physical volume \"/dev/mapper/metadata-data01p1\" successfully created Physical volume \"/dev/mapper/metadata-data02p1\" successfully created Physical volume \"/dev/mapper/metadata-data03p1\" successfully created Physical volume \"/dev/mapper/metadata-data04p1\" successfully created Physical volume \"/dev/mapper/metadata-data05p1\" successfully created Physical volume \"/dev/mapper/metadata-data06p1\" successfully created Physical volume \"/dev/mapper/metadata-data07p1\" successfully created Physical volume \"/dev/mapper/metalog-arch01p1\" successfully created Physical volume \"/dev/mapper/metalog-arch02p1\" successfully created Physical volume \"/dev/mapper/metalog-log01p1\" successfully created Physical volume \"/dev/mapper/metalog-log02p1\" successfully created Physical volume \"/dev/mapper/srcdata02p1\" successfully created VGCREATE!!! quote “Example” vgcreate vg_auditlog /dev/mapper/auditlogvg-log0[123]p1) EDS etlprod2 ~ # for vg in $(lvmdiskscan | grep p1 | awk &apos;&#123; print $1 &#125;&apos; | awk -F/ &apos;&#123; print $4 &#125;&apos; | sed &apos;s/-.*$//&apos; | sed &apos;s/02p1//&apos; | sort -n | uniq); do list=`ls /dev/mapper/$vg*p1`; vgcreate vg_$vg $list; done Volume group &quot;vg_metabackup&quot; successfully created Volume group &quot;vg_metadata&quot; successfully created Volume group &quot;vg_metalog&quot; successfully created Volume group &quot;vg_srcdata&quot; successfully created PARTPROBEUse partprobe to bring devices into /dev/mapper !!! question “Why partprobe?” http://www.redhat.com/advice/tips/rhce/partprobe.html One Achilles heel for Linux, until the past couple of years, has been the fact that the Linux kernel only reads partition table information at system initialization, necessitating a reboot any time you wish to add new disk partitions to a running system. The good news, however, is that disk re-partitioning can now also be handled ‘on-the-fly’ thanks to the ‘partprobe’ command, which is part of the ‘parted’ package. Using ‘partprobe’ couldn’t be more simple. Any time you use ‘fdisk’, ‘parted’ or any other favorite partitioning utility you may have to modify the partition table for a drive, run ‘partprobe’ after you exit the partitioning utility and ‘partprobe’ will let the kernel know about the modified partition table information. If you have several disk drives and want to specify a specific drive for ‘partprobe’ to scan, you can run ‘partprobe &lt;device_node&gt;’ Of course, given a particular hardware configuration, shutting down your system to add hardware may be unavoidable, it’s still nice to be given the option of not having to do so and ‘partprobe’ fills that niche quite nicely.” !!! info “RHEL 6 and partprobe” https://access.redhat.com/solutions/57542 partprobe was commonly used in RHEL 5 to inform the OS of partition table changes on the disk. In RHEL 6, it will only trigger the OS to update the partitions on a disk that none of its partitions are in use (e.g. mounted). If any partition on a disk is in use, partprobe will not trigger the OS to update partitions in the system because it is considered unsafe in some situations. So in general we would suggest: Unmount all the partitions of the disk before modifying the partition table on the disk, and then run partprobe to update the partitions in system. If this is not possible (e.g. the mounted partition is a system partition), reboot the system after modifying the partition table. The partitions information will be re-read after reboot. If a new partition was added and none of the existing partitions were modified, consider using the partx command to update the system partition table. Do note that the partx command does not do much checking between the new and the existing partition table in the system and assumes the user knows what they are are doing. So it can corrupt the data on disk if the existing partitions are modified or the partition table is not set correctly. So use at one’s own risk. For example, a partition #1 is an existing partition and a new partition #2 is already added in /dev/sdb by fdisk. Here we use partx -v -a /dev/sdb to add the new partition to the system: # ls /dev/sdb* /dev/sdb /dev/sdb1 List the partition table of disk: # partx -l /dev/sdb # 1: 63- 505007 ( 504945 sectors, 258 MB) # 2: 505008- 1010015 ( 505008 sectors, 258 MB) # 3: 0- -1 ( 0 sectors, 0 MB) # 4: 0- -1 ( 0 sectors, 0 MB) Read disk and try to add all partitions to the system: # partx -v -a /dev/sdb device /dev/sdb: start 0 size 2097152 gpt: 0 slices dos: 4 slices # 1: 63- 505007 ( 504945 sectors, 258 MB) # 2: 505008- 1010015 ( 505008 sectors, 258 MB) # 3: 0- -1 ( 0 sectors, 0 MB) # 4: 0- -1 ( 0 sectors, 0 MB) BLKPG: Device or resource busy error adding partition 1 The last 2 lines are normal in this case because partition 1 is already added in the system before partition 2 is added Check that we have device nodes for /dev/sdb itself and the partitions on it: # ls /dev/sdb* /dev/sdb /dev/sdb1 /dev/sdb2 LVCREATE(options)-L &lt;size in M or G (read man page)&gt;or-l &lt;pe divided by number of physical volumes to be included in said vg from vgdisplay &lt;vg&gt; or as defined per physical volume if different sizes for each pv is desired&gt; lvcreate -l 766 -n lv_log01 vg_auditlog create file systems and mountfor vg in $(vgs --noheadings --options vg_name | grep -v etlprod1); do for lv in $(ls -1 /dev/$vg); do echo &quot;mkfs.ext4 /dev/$vg/$lv&quot;; done; done for vg in $(vgs --noheadings --options vg_name | grep -v etlprod1); do for lv in $(ls -1 /dev/$vg); do mkfs.ext4 /dev/$vg/$lv; done; done for vg in $(vgs --noheadings --options vg_name | grep -v etlprod1); do for lv in $(ls -1 /dev/$vg); do echo &quot;/dev/$vg/$lv &lt;mpoint&gt; ext4 defaults 1 2&quot;; done; done edit fstab for fs in $(mount | grep mapper | grep -v etlprod1 | awk '&#123; print $3 &#125;'); do chown oracle:dba $fs; donefor fs in $(mount | grep mapper | grep -v etlprod1 | awk '&#123; print $3 &#125;'); do ls -ld $fs; done One day, I hope to clean this up a bit and layout the article in a more knowlede sharing format. As you can see, there is a bit of leg work involved in setting things up and I remember spending a few hours getting this all setup and figured things out as I went along, hence the cliff note. Again, it’s here for my benefit in reference material, and to share with anyone who may be performing a simliar setup.","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"How to Install Teradata Client 13.10","slug":"teradata-client","date":"2019-05-05T06:50:00.000Z","updated":"2019-05-07T01:54:23.000Z","comments":true,"path":"2019/05/05/teradata-client/","link":"","permalink":"http://kristianreese.com/2019/05/05/teradata-client/","excerpt":"","text":"How to install Teradata Client 13.10 64bit on RedHat 6 x86_64!!! note This does not cover installation from the CDROM media. The procedure below pertains to downloading the software from Teradata, and installing the packages. I found that installing the Teradata client on RedHat 6 wasn&apos;t intuitive, and the install document (from teradata knowledge article KAP1B923E) wasn&apos;t straight forward. Here are some considerations while preparing to install the client: Where do I download the software? Login to your account at Teradata Portal site Click on Software Downloads Click on “Teradata Client” Under “TD Client Platform”, click Linux Select software base. In this case, I’ve downloaded TTU 13.10 Select platform base. download 64bit x8664 download 32bit i386 (yes - download both as some of the 32bit packages are dependencies to installing the 64bit packages) Select Current and Submit Select all packages for download and download them via any of the options provided Note: Even though the page is subsequently labeled as “patches”. It’s the full client version. How to install the Teradata Client gunzip/untar the downloaded packages copy these packages into a new directory (I created a directory called tdclient). Note; The TTU 12.0 and 13.x packages for Linux work on either RedHat or SuSE. Although they’re labeled as specific to one or the other, each works on both. TeraGSS_redhatlinux-i386-13.10.06.01-1.i386.rpm TeraGSS_suselinux-x8664-13.10.06.01-1.x86_64.rpm tdicu-13.10.00.02-1.noarch.rpm tdodbc-13.10.00.09-1.noarch.rpm cliv2-13.10.00.13-1.noarch.rpm piom-13.10.00.10-1.noarch.rpm npaxsmod-13.10.00.02-1.i386.rpm bteq-13.10.00.10-1.i386.rpm fastexp-13.10.00.15-1.i386.rpm fastld-13.10.00.16-1.i386.rpm mload-13.10.00.10-1.i386.rpm tpump-13.10.00.10-1.i386.rpm If not installed, install the following packages before proceeding ksh compat-libstdc++-33.i686 create an install script as follows. Install Procedure#!/bin/kshif [ ! -f /bin/ksh ]; then echo \"Please install ksh before proceeding: 'yum install ksh' exiting...\" exit 1else if [ -L /usr/bin/ksh ]; then echo ok else echo \"soft linking /bin/ksh -&gt; /usr/bin/ksh\" ln -s /bin/ksh /usr/bin/ksh echo ok fifirpm -q compat-libstdc++-33 &gt;/dev/null 2&gt;&amp;1if [ $? -eq 1 ]; then echo \"Please install compat-libstdc++-33 before proceeding: 'yum install compat-libstdc++-33.i686' exiting...\" exit 1firpm -ivh TeraGSS_redhatlinux-i386-13.10.06.01-1.i386.rpmrpm -ivh TeraGSS_suselinux-x8664-13.10.06.01-1.x86_64.rpmrpm -ivh tdicu-13.10.00.02-1.noarch.rpmrpm -ivh tdodbc-13.10.00.09-1.noarch.rpmrpm -ivh cliv2-13.10.00.13-1.noarch.rpmrpm -ivh piom-13.10.00.10-1.noarch.rpmrpm -ivh npaxsmod-13.10.00.02-1.i386.rpmrpm -ivh bteq-13.10.00.10-1.i386.rpmrpm -ivh fastexp-13.10.00.15-1.i386.rpmrpm -ivh fastld-13.10.00.16-1.i386.rpmrpm -ivh mload-13.10.00.10-1.i386.rpmrpm -ivh tpump-13.10.00.10-1.i386.rpm First, we check if ksh is installed. The script will actually fail once it attempts to interpret the shebang if ksh isn’t in the specified location, so you may need to adjust some of the syntax accordingly if ksh is somewhere else besides /bin. Further into the script, a soft link to /usr/bin/ksh is created because the tdodbc package looks for ksh in /usr/bin. If all checks out, packages are installed ==in the order of which they need to be installed==. I put this together rather quickly so there are other ways to invoke the prerequisities, but it works fine for the purposes of getting to the main objective. If something messes up, the packages need to be uninstalled before you can reinstall the packages. See uninstall procedure at the end of this article. Here is the output after running a successful install: # ./install.kshPreparing... ########################################### [100%] 1:TeraGSS_redhatlinux-i38########################################### [100%] Output has been written to Binary file \"/usr/teragss/redhatlinux-i386/13.10.06.01/bin/../etc/tdgssconfig.bin\" Preparing... ########################################### [100%] 1:TeraGSS_suselinux-x8664########################################### [100%] Output has been written to Binary file \"/usr/teragss/suselinux-x8664/13.10.06.01/bin/../etc/tdgssconfig.bin\" Preparing... ########################################### [100%] 1:tdicu ########################################### [100%]Adding TD_ICU_DATA environment variable to /etc/profile file.Adding TD_ICU_DATA environment variable to /etc/csh.login file.Preparing... ########################################### [100%] 1:tdodbc ########################################### [100%]set_default_version -b 32 -v 13.10 -i /optADDING SYMBOLIC LINK: source: /opt/teradata/client/ODBC_32/lib target: /opt/teradata/client/13.10/odbc_32/libsymbolic link trail:[1] +-&gt; [/opt/teradata/client/ODBC_32/lib][2] +-&gt; [/opt/teradata/client/13.10/odbc_32/lib: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/ODBC_32/locale target: /opt/teradata/client/13.10/odbc_32/localesymbolic link trail:[1] +-&gt; [/opt/teradata/client/ODBC_32/locale][2] +-&gt; [/opt/teradata/client/13.10/odbc_32/locale: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/ODBC_32/include target: /opt/teradata/client/13.10/odbc_32/includesymbolic link trail:[1] +-&gt; [/opt/teradata/client/ODBC_32/include][2] +-&gt; [/opt/teradata/client/13.10/odbc_32/include: directory].build_pre130_bridge -b 32 -i /optADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_32_PRE130_BRIDGE/drivers/tdata.so target: /opt/teradata/client/ODBC_32/lib/tdata.sosymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_32_PRE130_BRIDGE/drivers/tdata.so][2] +-&gt; [/opt/teradata/client/ODBC_32/lib/tdata.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]ADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_32_PRE130_BRIDGE/lib target: /opt/teradata/client/ODBC_32/lib\\csymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_32_PRE130_BRIDGE/lib][2] +-&gt; [/opt/teradata/client/ODBC_32/lib][3] +-&gt; [/opt/teradata/client/13.10/odbc_32/lib: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_32_PRE130_BRIDGE/locale target: /opt/teradata/client/ODBC_32/locale\\csymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_32_PRE130_BRIDGE/locale][2] +-&gt; [/opt/teradata/client/ODBC_32/locale][3] +-&gt; [/opt/teradata/client/13.10/odbc_32/locale: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_32_PRE130_BRIDGE/odbcinst.ini target: /opt/teradata/client/ODBC_32/odbcinst.inisymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_32_PRE130_BRIDGE/odbcinst.ini][2] +-&gt; [/opt/teradata/client/ODBC_32/odbcinst.ini: ASCII text]enable_legacy_installdir -b 32 -i /opt -I /usr/odbcADDING SYMBOLIC LINK: source: /usr/odbc target: /opt/teradata/client/.ODBC_32_PRE130_BRIDGEsymbolic link trail:[1] +-&gt; [/usr/odbc][2] +-&gt; [/opt/teradata/client/.ODBC_32_PRE130_BRIDGE: directory]symbolic link trail:[1] +-&gt; [/usr/odbc/drivers/tdata.so][2] +-&gt; [/opt/teradata/client/ODBC_32/lib/tdata.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/odbcinst.ini][2] +-&gt; [/opt/teradata/client/ODBC_32/odbcinst.ini: ASCII text]symbolic link trail:[1] +-&gt; [/usr/odbc/locale/en_US/LC_MESSAGES/odbc.po: ASCII English text, with very long lines]symbolic link trail:[1] +-&gt; [/usr/odbc/locale/en_US/LC_MESSAGES/odbc.m: GLF_BINARY_LSB_FIRST]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/libtdparse.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/vscnctdlg.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/odbccurs.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/libtdsso.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/odbctrac.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/tdata.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/libodbc.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/tdconndlg.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/libodbcinst.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/usr/odbc/lib/libivicu24.so: ELF 32-bit LSB shared object, Intel 80386, version 1 (SYSV), dynamically linked, not stripped]set_default_version -b 64 -v 13.10 -i /optADDING SYMBOLIC LINK: source: /opt/teradata/client/ODBC_64/lib target: /opt/teradata/client/13.10/odbc_64/libsymbolic link trail:[1] +-&gt; [/opt/teradata/client/ODBC_64/lib][2] +-&gt; [/opt/teradata/client/13.10/odbc_64/lib: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/ODBC_64/locale target: /opt/teradata/client/13.10/odbc_64/localesymbolic link trail:[1] +-&gt; [/opt/teradata/client/ODBC_64/locale][2] +-&gt; [/opt/teradata/client/13.10/odbc_64/locale: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/ODBC_64/include target: /opt/teradata/client/13.10/odbc_64/includesymbolic link trail:[1] +-&gt; [/opt/teradata/client/ODBC_64/include][2] +-&gt; [/opt/teradata/client/13.10/odbc_64/include: directory].build_pre130_bridge -b 64 -i /optADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_64_PRE130_BRIDGE/drivers/tdata.so target: /opt/teradata/client/ODBC_64/lib/tdata.sosymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_64_PRE130_BRIDGE/drivers/tdata.so][2] +-&gt; [/opt/teradata/client/ODBC_64/lib/tdata.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]ADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_64_PRE130_BRIDGE/lib target: /opt/teradata/client/ODBC_64/lib\\csymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_64_PRE130_BRIDGE/lib][2] +-&gt; [/opt/teradata/client/ODBC_64/lib][3] +-&gt; [/opt/teradata/client/13.10/odbc_64/lib: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_64_PRE130_BRIDGE/locale target: /opt/teradata/client/ODBC_64/locale\\csymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_64_PRE130_BRIDGE/locale][2] +-&gt; [/opt/teradata/client/ODBC_64/locale][3] +-&gt; [/opt/teradata/client/13.10/odbc_64/locale: directory]ADDING SYMBOLIC LINK: source: /opt/teradata/client/.ODBC_64_PRE130_BRIDGE/odbcinst.ini target: /opt/teradata/client/ODBC_64/odbcinst.inisymbolic link trail:[1] +-&gt; [/opt/teradata/client/.ODBC_64_PRE130_BRIDGE/odbcinst.ini][2] +-&gt; [/opt/teradata/client/ODBC_64/odbcinst.ini: ASCII text]enable_legacy_installdir -b 64 -i /opt -I /opt/teradata/client/odbcADDING SYMBOLIC LINK: source: /opt/teradata/client/odbc target: /opt/teradata/client/.ODBC_64_PRE130_BRIDGEsymbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc][2] +-&gt; [/opt/teradata/client/.ODBC_64_PRE130_BRIDGE: directory]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/drivers/tdata.so][2] +-&gt; [/opt/teradata/client/ODBC_64/lib/tdata.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/odbcinst.ini][2] +-&gt; [/opt/teradata/client/ODBC_64/odbcinst.ini: ASCII text]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/locale/en_US/LC_MESSAGES/odbc.po: ASCII English text, with very long lines]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/locale/en_US/LC_MESSAGES/odbc.m: GLF_BINARY_LSB_FIRST]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/libtdparse.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/vscnctdlg.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/odbccurs.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/libddicu24.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/libtdsso.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/odbctrac.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/tdata.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/libodbc.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/tdconndlg.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped]symbolic link trail:[1] +-&gt; [/opt/teradata/client/odbc/lib/libodbcinst.so: ELF 64-bit LSB shared object, x86-64, version 1 (SYSV), dynamically linked, not stripped] ============================================================================ *** IMPORTANT ODBC USER BULLETIN *** ============================================================================ The TTU 13.0 release has introduced a new TTU directory tree structure which has unified all the TTU products under a common TTU release directory. The ODBC 13.0 installation package has taken steps to make this new tree structure transparent to the existing ODBC users that have selected the default install directory when installing an ODBC release prior to 13.0. In the case where the user has selected a non-default install directory, the user will need to follow the instructions that are documented in the ODBC README file. In either case, it is strongly recommended that the user read the ODBC README file to understand the new TTU tree structure and how it relates to the ODBC Driver and its odbc.ini and odbcinst.ini files. ============================================================================ *** IMPORTANT ODBC USER BULLETIN *** ============================================================================Preparing... ########################################### [100%] 1:cliv2 ########################################### [100%]Adding cliv2 COPLIB environment variable to /etc/profile file.Adding cliv2 COPERR environment variable to /etc/profile file.Adding tdmst entry to /etc/services file.Preparing... ########################################### [100%] 1:piom ########################################### [100%]Preparing... ########################################### [100%] 1:npaxsmod ########################################### [100%]Preparing... ########################################### [100%] 1:bteq ########################################### [100%]Preparing... ########################################### [100%] 1:fastexp ########################################### [100%]Preparing... ########################################### [100%] 1:fastld ########################################### [100%]Preparing... ########################################### [100%] 1:mload ########################################### [100%]Preparing... ########################################### [100%] 1:tpump ########################################### [100%] Uninstall Procedurerpm -e tpump-13.10.00.10-1.i386rpm -e tdodbc-13.10.00.09-1.noarchrpm -e bteq-13.10.00.10-1.i386rpm -e mload-13.10.00.10-1.i386rpm -e fastld-13.10.00.16-1.i386rpm -e fastexp-13.10.00.15-1.i386rpm -e cliv2-13.10.00.13-1.noarchrpm -e tdicu-13.10.00.02-1.noarchrpm -e TeraGSS_suselinux-x8664-13.10.06.01-1.x86_64rpm -e TeraGSS_redhatlinux-i386-13.10.06.01-1.i386rpm -e piom-13.10.00.10-1.noarchrpm -e npaxsmod-13.10.00.02-1.i386rm -rf /opt/teradata","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Add and Delete null routes","slug":"add-and-delete-a-null-route","date":"2019-05-05T06:49:00.000Z","updated":"2019-05-05T02:58:06.000Z","comments":true,"path":"2019/05/05/add-and-delete-a-null-route/","link":"","permalink":"http://kristianreese.com/2019/05/05/add-and-delete-a-null-route/","excerpt":"","text":"Simple set of commands for how to add and delete a null route: Adding a null routeroute add &lt;ip address&gt; gw 127.0.0.1 Deleting a null routeroute delete &lt;ip address&gt; gw 127.0.0.1","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Add Disk without Rebooting","slug":"add-disk-without-rebooting","date":"2019-05-05T06:49:00.000Z","updated":"2019-05-07T01:44:56.000Z","comments":true,"path":"2019/05/05/add-disk-without-rebooting/","link":"","permalink":"http://kristianreese.com/2019/05/05/add-disk-without-rebooting/","excerpt":"","text":"How to add a disk to a Linux System without rebootingIt is possible to add a new hard disk without a reboot. Whether or not it is a physical server where a physical disk was inserted into an available slot, or a new virtual disk was added to a VMware VM, the command structure to discover the new disk is the same. First, take a look at the existing scsi devices using the lsscsi command: ~# lsscsi[0:0:0:0] disk VMware Virtual disk 1.0 /dev/sda[0:0:1:0] disk VMware Virtual disk 1.0 /dev/sdb After adding a new vmdk or a new physical disk, /dev/sdc should pop in after running through the following sequence of commands in order to make the operating system aware of the new storage device. Adding a deviceThe recommended command to do this, as per redhat documentation, is: echo \"c t l\" &gt; /sys/class/scsi_host/hosth/scan From the lsscsi output above, this would translate into: echo \"0 2 0\" &gt; /sys/class/scsi_host/host0/scan where host 0, channel 0, SCSI target ID 2, and LUN 0 are the next in-line / appropriate values: The deprecated method was to submit a scsi command to /proc/scsi/scsi as follows, but does still work: echo \"scsi add-single-device 0 0 2 0\" &gt; /proc/scsi/scsi If that’s not a comfortable option, then a rescan of the entire scsi bus for additional disks can be performed. Some systems have more than one scsi_host. With one command, they can all be rescanned as follows: for host in $(ls -1d /sys/class/scsi_host/*); do echo \"- - -\" &gt; $&#123;host&#125;/scan ; done To rescan them individually, perform an ls in /sys/class/scsi_host and scan each one via a single command where # is the host number: echo \"- - -\" &gt; /sys/class/scsi_host/host#/scan Note that in some distros of Linux, the kernel may recognize the new device once it has been added to the system, and the bus scan may not need to be performed. This would be validated by observing the output of lssci or by looking in syslog (/var/log/messages) for any indication of a newly added device. Something like the following would show up in the logs: sd 0:0:2:0: Attached scsi disk sdc This matches up nicely with lsscsi output: ~# lsscsi[0:0:0:0] disk VMware Virtual disk 1.0 /dev/sda[0:0:1:0] disk VMware Virtual disk 1.0 /dev/sdb[0:0:2:0] disk VMware Virtual disk 1.0 /dev/sdc Next, the new device can be partitioned, labeled, create a file system, and mounted. echo -e -n \"o\\nn\\np\\n1\\n\\n\\nw\\n\" | fdisk /dev/sdc &gt; /dev/null 2&gt;&amp;1mkfs.ext4 /dev/sdc1mkdir /datae2label /dev/sdc1 /datavi /etc/fstabmount -a Deleting a device!!! note devName is the device name to be removed: echo 1 &gt; /sys/block/devName/device/delete For example, using sdc as devName: echo 1 &gt; /sys/block/sdc/device/delete Another variation of this operation is: echo 1 &gt; /sys/class/scsi_device/h:c:t:l/device/delete A deprecated method of the above command is: echo &quot;scsi remove-single-device 0 0 2 0&quot; &gt; /proc/scsi/scsi","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"},{"title":"Add New LUN via EMC powerpath","slug":"add-new-lun-emc-powerpath","date":"2019-05-05T06:49:00.000Z","updated":"2019-05-07T01:45:49.000Z","comments":true,"path":"2019/05/05/add-new-lun-emc-powerpath/","link":"","permalink":"http://kristianreese.com/2019/05/05/add-new-lun-emc-powerpath/","excerpt":"","text":"How to add a new LUN using EMC Navisphere and powerpathOverview Create a new LUN on an EMC Clariion CX3-40 via Navisphere Assign LUN to proper Storage Group Use powermt to bring LUN in and configure it Rescan scsi bus on Linux server for new LUN Bring new LUN under LVM control and create file system Create a new LUN on an EMC Clariion CX3-40 via Navisphere Login to Navisphere. Within the Enterprise Storage window, expand the Storage Domains tree until the RAID Groups are displayed Right click on the RAID Group that’s to contain the new LUN Choose the option Bind LUN... The window below will display. Fill it out as shown or with the desired options Click Apply. Confirmation dialogs will appear. Assign LUN to proper Storage Group Next, assign the LUN to the desired storage group by right clicking on the new LUN and choosing the option to “Add to Storage Group” Click OK and confirm. Use powermt to bring LUN in and configure itNow that the LUN has been created, it’s time to discover it on the server. Using the PowerPath Management Utility powermt, we accomplish this by running these commands: # powermt config# powermt display dev=all If the new LUN is not showing, check that both paths are healthy with the powermt display command. If degraded, you may not be able to discover the new LUN. This happened to me and I had to reboot the machine to clear everything up, which returned both paths to an optimal state. I could have tried powremt restore which performs an I/O path check and will mark alive any previous paths marked as dead, but I took the easy way out ;) Rescan scsi bus on Linux server for new LUNIf, after running powermt config the LUN is still not visible, you may rescan the bus: ~# for host in $(ls -1d /sys/class/fc_host/*); do echo \"1\" &gt; $&#123;host&#125;/issue_lip; done ~# for host in $(ls -1d /sys/class/scsi_host/*); do echo \"- - -\" &gt; $&#123;host&#125;/scan ; done and try powermt config followed by a powermt display to see the new LUN. Bring new LUN under LVM control and create file systemOnce you know what block device is associated with the LUN as seen in the powermt display dev=all output, the following series of command will be used to bring the disk under LVM: fdisk /dev/emcpowerag pvcreate /dev/emcpowerag1 vgcreate vg_dwstore /dev/emcpowerag1 vgchange --addtag dwetlprod2 vg_dwstore lvcreate -l 25599 -n lv_dwstore vg_dwstore lvchange -ay vg_dwstore/lv_dwstore mkfs.ext3 /dev/vg_dwstore/lv_dwstore mkdir /dwstore mount /dev/vg_dwstore/lv_dwstore /dwstore [root@dwetlprod2 bin]# fdisk /dev/emcpoweragDevice contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabelBuilding a new DOS disklabel. Changes will remain in memory only,until you decide to write them. After that, of course, the previouscontent won't be recoverable.The number of cylinders for this disk is set to 13054.There is nothing wrong with that, but this is larger than 1024,and could in certain setups cause problems with:1) software that runs at boot time (e.g., old versions of LILO)2) booting and partitioning software from other OSs (e.g., DOS FDISK, OS/2 FDISK)Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)Command (m for help): pDisk /dev/emcpowerag: 107.3 GB, 107374182400 bytes255 heads, 63 sectors/track, 13054 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id SystemCommand (m for help): nCommand action e extended p primary partition (1-4)pPartition number (1-4): 1First cylinder (1-13054, default 1): Using default value 1Last cylinder or +size or +sizeM or +sizeK (1-13054, default 13054): Using default value 13054Command (m for help): tSelected partition 1Hex code (type L to list codes): 8eChanged system type of partition 1 to 8e (Linux LVM)Command (m for help): wThe partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks.[root@dwetlprod2 bin]# [root@dwetlprod2 bin]# pvcreate /dev/emcpowerag1 Physical volume \"/dev/emcpowerag1\" successfully created[root@dwetlprod2 bin]# vgcreate vg_dwstore /dev/emcpowerag1 Volume group \"vg_dwstore\" successfully created[root@dwetlprod2 bin]# pvdisplay /dev/emcpowerag1 --- Physical volume --- PV Name /dev/emcpowerag1 VG Name vg_dwstore PV Size 100.00 GB / not usable 0 Allocatable yes PE Size (KByte) 4096 Total PE 25599 Free PE 25599 Allocated PE 0 PV UUID VkvFFP-qBlo-1U6A-JDXS-nceB-GOCy-xpZH1R [root@dwetlprod2 bin]# [root@dwetlprod2 bin]# lvcreate -l 25599 -n lv_dwstore vg_dwstore Failed to activate new LV. !!! error “Failed to activate new LV” I have host tags enabled in lvm.conf, so I must tag the volume group to that of the hostname! [root@dwetlprod2 bin]# vgchange --addtag dwetlprod2 vg_dwstore Volume group \"vg_dwstore\" successfully changed[root@dwetlprod2 bin]# lvchange -ay vg_dwstore/lv_dwstore[root@dwetlprod2 bin]# [root@dwetlprod2 bin]# mkfs.ext3 /dev/vg_dwstore/lv_dwstore mke2fs 1.35 (28-Feb-2004)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)13107200 inodes, 26213376 blocks1310668 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=4294967296800 block groups32768 blocks per group, 32768 fragments per group16384 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872Writing inode tables: done Creating journal (8192 blocks): doneWriting superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 34 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override.[root@dwetlprod2 bin]# mkdir /dwstore[root@dwetlprod2 bin]# mount /dev/vg_dwstore/lv_dwstore /dwstore[root@dwetlprod2 bin]# df -Ph /dwstoreFilesystem Size Used Avail Use% Mounted on/dev/mapper/vg_dwstore-lv_dwstore 99G 92M 94G 1% /dwstore Put entry in /etc/fstab and it’s done!","categories":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/categories/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://kristianreese.com/tags/Linux/"}],"author":"Kris Reese"}]}